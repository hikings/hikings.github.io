{"meta":{"title":"Life is like hiking","subtitle":"Keep It Simple,Stupid!","description":"��ѧ�����ʡ���˼�����桢����","author":"Kendall.j.","url":"http://hikings.github.io"},"pages":[{"title":"categories","date":"2017-09-23T13:51:50.000Z","updated":"2017-09-23T13:52:18.000Z","comments":false,"path":"categories/index.html","permalink":"http://hikings.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-09-23T13:49:48.000Z","updated":"2017-09-23T13:51:34.000Z","comments":false,"path":"tags/index.html","permalink":"http://hikings.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Sublime Text 3 插入日期时间","slug":"2017-09-22_Sublime_Text_3_insert_date","date":"2017-09-22T15:52:50.000Z","updated":"2017-09-22T16:59:30.000Z","comments":true,"path":"2017/09/22/2017-09-22_Sublime_Text_3_insert_date/","link":"","permalink":"http://hikings.github.io/2017/09/22/2017-09-22_Sublime_Text_3_insert_date/","excerpt":"Sublime Text 并没有自带插入日期时间的功能，但是可以通过插件的方式实现，这也是sublime可扩展性的强大体现。详细的步骤如下： 1. 菜单栏：Tools -&gt; New Plugin... 2. 此时会打开一个插件模版文件，使用下面的内容覆盖","text":"Sublime Text 并没有自带插入日期时间的功能，但是可以通过插件的方式实现，这也是sublime可扩展性的强大体现。详细的步骤如下： 1. 菜单栏：Tools -&gt; New Plugin... 2. 此时会打开一个插件模版文件，使用下面的内容覆盖 1234567891011121314151617181920212223242526import datetime, getpassimport sublime, sublime_pluginimport datetimeclass AddDateTimeStampCommand(sublime_plugin.TextCommand): def run(self, edit): self.view.run_command(\"insert_snippet\", &#123; # \"contents\": \"%s\" % datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S %A\") # 可根据自己的需要进行调整（参照后面的日期时间格式） \"contents\": \"/**\"\"\\n\" \" * \"\"\\n\" \" * @author: author\"\"\\n\" \" * @dateTime: \" \"%s\" %datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +\"\\n\" \" * @description: \"\"\\n\" \" */\" &#125; )``` 3. ctrl+s保存文件，输入文件名 add_date.py，可以随便定义，然后确认一下保存的位置是 Packages/User/add_date.py 4. 绑定快捷键，在菜单栏：Preferences -&gt; Key Bindings -&gt; User，使用下面的内容覆盖```python[ &#123;\"keys\": [\"ctrl+shift+f5\"], \"command\": \"add_date_time_stamp\" &#125;] 5. 保存上一步的快捷键设置，在需要插入的地方使用快捷键试试吧！效果如下： 123456789/** * * @author: author * @dateTime: 2015-08-12 11:42:34 * @description: */ class Demo &#123; &#125; 附录（strftime的格式）： %a 星期简称 %A 星期全称 %b 月份简称 %B 月份全称 %c 标准的日期的时间串 %C 年份的后两位数字 %d 十进制表示的每月的第几天 %D 月/天/年 %e 在两字符域中，十进制表示的每月的第几天 %F 年-月-日 %g 年份的后两位数字，使用基于周的年 %G 年份，使用基于周的年 %h 简写的月份名 %H 24小时制的小时 %I 12小时制的小时 %j 十进制表示的每年的第几天 %m 十进制表示的月份 %M 十时制表示的分钟数 %n 新行符 %p 本地的AM或PM的等价显示 %r 12小时的时间 %R 显示小时和分钟：hh:mm %S 十进制的秒数 %t 水平制表符 %T 显示时分秒：hh:mm:ss %u 每周的第几天，星期一为第一天 （值从0到6，星期一为0） %U 第年的第几周，把星期日作为第一天（值从0到53） %V 每年的第几周，使用基于周的年 %w 十进制表示的星期几（值从0到6，星期天为0） %W 每年的第几周，把星期一做为第一天（值从0到53） %x 标准的日期串 %X 标准的时间串 %y 不带世纪的十进制年份（值从0到99） %Y 带世纪部分的十制年份 %z，%Z 时区名称，如果不能得到时区名称则返回空字符。 %% 百分号","categories":[{"name":"Tips","slug":"Tips","permalink":"http://hikings.github.io/categories/Tips/"}],"tags":[{"name":"sublimeText","slug":"sublimeText","permalink":"http://hikings.github.io/tags/sublimeText/"}]},{"title":"Netflix 开源项目推荐","slug":"2017-09-22-netflix-open-source","date":"2017-09-22T14:52:50.000Z","updated":"2017-09-22T16:59:57.000Z","comments":true,"path":"2017/09/22/2017-09-22-netflix-open-source/","link":"","permalink":"http://hikings.github.io/2017/09/22/2017-09-22-netflix-open-source/","excerpt":"联合作业执行引擎 Netflix Geniehttps://www.oschina.net/p/netflix-genieGenie 是 Netflix 联合作业执行引擎，提供 REST-ful APIs，运行各种类型的大数据作业，比如 Hadoop，Pig，Hive，Spark，Presto，Sqoop 等等。Genie 同时提供 APis 来管理在上面运行的大量的分布式进程集群配置，命令和应用。 基于 Java 的 Mesos 调度器 Fenzohttps://www.oschina.net/p/fenzoFenzo 是一个使用 Java 语言编写的 Apache Mesos 框架的可扩展调度器。Fenzo 负责管理 Netflix 内部所有服务的调度和资源分配。Fenzo 支持使用两种互补的策略集群自动缩放： ● 基于阈值 ● 基于资源短缺分析","text":"联合作业执行引擎 Netflix Geniehttps://www.oschina.net/p/netflix-genieGenie 是 Netflix 联合作业执行引擎，提供 REST-ful APIs，运行各种类型的大数据作业，比如 Hadoop，Pig，Hive，Spark，Presto，Sqoop 等等。Genie 同时提供 APis 来管理在上面运行的大量的分布式进程集群配置，命令和应用。 基于 Java 的 Mesos 调度器 Fenzohttps://www.oschina.net/p/fenzoFenzo 是一个使用 Java 语言编写的 Apache Mesos 框架的可扩展调度器。Fenzo 负责管理 Netflix 内部所有服务的调度和资源分配。Fenzo 支持使用两种互补的策略集群自动缩放： ● 基于阈值 ● 基于资源短缺分析 持续交付平台 Spinnakerhttps://www.oschina.net/p/spinnakerSpinnaker 是一个持续交付平台，它定位于将产品快速且持续的部署到多种云平台上。Spinnaker 主要特性：配置一次，随时运行；随地部署，集中化管理；开源。Spinnaker 组件： 云操作容错解决方案 SimianArmyhttps://www.oschina.net/p/simianarmySimianArmy 让你的云操作保持最佳状态的工具。Chaos Monkey 是伸缩性很强的工具，是 SimianArmy 的第一个成员，能帮助应用更好的容忍随机故障实例。Simian Army 是 Netflix 实例的随机故障测试，用于验证可靠性。 分布式系统的延迟和容错库 Hystrixhttps://www.oschina.net/p/hystrixNetflix（一家在线影片租赁商）近日开源了其Hystrix库，这是一个针对分布式系统的延迟和容错库。Hystrix 供分布式系统使用，提供延迟和容错功能，隔离远程系统、访问和第三方程序库的访问点，防止级联失败，保证复杂的分布系统在面临不可避免的失败时，仍能有其弹性。 数据管道服务 Surohttps://www.oschina.net/p/suroNetflix 开源了一个叫做Suro的工具，它能够在数据被发送到不同的数据平台（如Hadoop、Elasticsearch）之前，收集不同应用服务器上的事件数据，这项创新技术具备成为大数据主流实践的潜力Suro 是数据管道服务，用来收集，聚合和调度大数据应用事件，包括日志记录数据。 分布式内存数据云存储 EVCachehttps://www.oschina.net/p/evcacheEVCache 是基于 memcached &amp; spymemcached 的缓存解决方案，主要用在 AWS EC2 基础设施上，为了顺畅的使用数据而开发的缓存解决方案。EVCache 是一个缩写的单词，包含： ● Ephemeral - 数据存储是短暂的，有自身的 TTL (Time To Live). ● Volatile - 数据可以在任何时候消失 ● Cache - 一个内存键值存储 云端负载均衡工具 Eurekahttps://www.oschina.net/p/eurekaNetflix 填补了Amazon Web Services的大缺口，发布了云端负载均衡工具Eureka。Netflix通过开源工具让亚马逊的服务变得更可靠。亚马逊提供了一个负载均衡工具Elastic Load Balancer，但针对的是终端用户Web流量服务器，而Eureka针对的是中间层服务器的负载均衡。","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://hikings.github.io/categories/MachineLearning/"}],"tags":[{"name":"netflix","slug":"netflix","permalink":"http://hikings.github.io/tags/netflix/"},{"name":"opensourcei","slug":"opensourcei","permalink":"http://hikings.github.io/tags/opensourcei/"}]},{"title":"Install python27 on centos6","slug":"2016-10-15-Install_python27_centos","date":"2016-10-07T14:01:33.000Z","updated":"2017-09-27T15:04:58.000Z","comments":true,"path":"2016/10/07/2016-10-15-Install_python27_centos/","link":"","permalink":"http://hikings.github.io/2016/10/07/2016-10-15-Install_python27_centos/","excerpt":"在工作环境中使用的是python 2.7.*，但是CentOS 6.4中默认使用的python版本是2.6.6，故需要升级版本。","text":"在工作环境中使用的是python 2.7.*，但是CentOS 6.4中默认使用的python版本是2.6.6，故需要升级版本。 安装步骤如下：1 先安装GCC，用如下命令yum install gcc gcc-c++ 2 下载python-2.7.5.tar.gz文件，修改文件权限chmode +x python-7.5.tar.gz 3 解压tar文件，tar -xzvf python-2.7.5.tar.gz 4 cd python-2.7.5 ./configure –prefix=/usr/local/python27 ### 注意：安装在新目录，防止覆盖系统默认安装的python make &amp;&amp; make install 5、建立软连接，使系统默认的python指向python27 mv /usr/bin/python /usr/bin/python2.6.6.old ln -s /usr/local/bin/python27 /usr/bin/python 已经安装完成python的安装或升级的全部操作了，我们再来看一下现在的python的版本： -V```1```Python 2.7.5 虽然现在python已经安装完成，但是使用yum命令会有问题——yum不能正常工作： yum list```123456789101112131415161718There was a problem importing one of the Python modulesrequired to run yum. The error leading to this problem was:No module named yumPlease install a package which provides this module orverify that the module is installed correctly.It&apos;s possible that the above module doesn&apos;t match thecurrent version of Python这是因为yum默认使用的python版本是2.6.6，到哪是现在的python版本是2.7.5，故会出现上述问题，只需要该一下yum的默认python配置版本就行了：```#vi /usr/bin/yum 将文件头部的#!/usr/bin/python改为 123#!/usr/bin/python2.6https://pypi.python.org/pypi/setuptoolswget https://bootstrap.pypa.io/ez_setup.py -O - | python Centos 6.4 安装Python 2.7 python-pip准备工作下载源码包 wget http://python.org/ftp/python/2.7.3/Python-2.7.3.tar.bz2查看是否安装make工具 ~#rpm -qa|grep makeautomake-1.11.1-4.el6.noarchmake-3.81-20.el6.x86_64如果没有安装make工具 yum -y install gcc automake autoconf libtool make查看是否安装zlib库 ~#rpm -qa|grep zlibzlib-devel-1.2.3-29.el6.x86_64zlib-1.2.3-29.el6.x86_64安装zlib yum install zlib-devel检查是否安装ssl 库 123456~#rpm -qa|grep opensslopenssl-devel-1.0.1e-16.el6_5.x86_64openssl-static-1.0.1e-16.el6_5.x86_64openssl098e-0.9.8e-17.el6.centos.2.x86_64openssl-1.0.1e-16.el6_5.x86_64openssl-perl-1.0.1e-16.el6_5.x86_64 安装openssl yum install openssl*安装bzip2依赖库 yum install -y bzip2* 编译安装cp Python-2.7.3.tar.bz2 /usr/src/tar -jxvf Python-2.7.3.tar.bz2vi Modules/Setup.dist 找到 123456#SSL=/usr/local/ssl#_ssl _ssl.c \\# -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\# -L$(SSL)/lib -lssl -lcrypto......#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz 把注释去掉 编译 ./configure –prefix=/usr/local/python2.7make allmake installmake cleanmake distclean 安装成功 ~#/usr/local/python2.7/bin/python2.7Python 2.7.3 (default Dec 18 2013 15:43:35)[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2Type “help” “copyright” “credits” or “license” for more information. cd /usr/local/python2.7python setup.py install建立python2.7 软链 ~#mv /usr/bin/python /usr/bin/python.bak~#ln -s /usr/local/python2.7/bin/python2.7 /usr/bin/python2.7~#ln -s /usr/bin/python2.7 /usr/bin/python 解决yum无法使用的问题 1234567891011121314151617~#yum updateThere was a problem importing one of the Python modulesrequired to run yum. The error leading to this problem was: No module named yumPlease install a package which provides this module orverify that the module is installed correctly.It&apos;s possible that the above module doesn&apos;t match thecurrent version of Python which is:2.7.3 (default Dec 18 2013 15:43:35) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)]If you cannot solve this problem yourself please go to the yum faq at: http://yum.baseurl.org/wiki/Faq 因为centos 6.4 下yum默认使用的是python2.6 12345678vi /usr/bin/yum----------------------------------------------------#!/usr/bin/pythonimport systry: import yumexcept ImportError:....... 修改为 #!/usr/bin/python2.6…….. 安装python-pip工具 先安装setup-tools wget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg –no-check-certificatechmod +x setuptools-0.6c11-py2.7.eggsh setuptools-0.6c11-py2.7.egg 安装pip 123456wget https://pypi.python.org/packages/source/p/pip/pip-1.3.1.tar.gz --no-check-certificatecp pip-1.3.1.tar.gz /usr/src/tar zxvf pip-1.3.1.tar.gzcd pip-1.3.1python setup.py installln -s /usr/local/python2.7/bin/pip /usr/bin/pip","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://hikings.github.io/categories/MachineLearning/"}],"tags":[{"name":"Centos","slug":"Centos","permalink":"http://hikings.github.io/tags/Centos/"},{"name":"Python","slug":"Python","permalink":"http://hikings.github.io/tags/Python/"}]},{"title":"谷歌论文 Borg、Omega and Kubernetes","slug":"2016-10-07-borg_omega_and_kubernetes","date":"2016-10-07T04:01:33.000Z","updated":"2016-10-07T12:37:18.000Z","comments":true,"path":"2016/10/07/2016-10-07-borg_omega_and_kubernetes/","link":"","permalink":"http://hikings.github.io/2016/10/07/2016-10-07-borg_omega_and_kubernetes/","excerpt":"google论文系列Borg、Omega and Kubernetes谷歌十余年从三个容器管理系统中得到的经验教训Lessons learned from three container-management systems over a decade 英文版原文链接 中文翻译参考了韩佳瑶的翻译并做了修正 &gt; 从2000年以来，谷歌基于容器研发三个容器管理系统，分别是Borg、Omega和Kubernetes。这篇论文由这三个容器集群管理系统长年开发维护的谷歌工程师Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer和John Wilkes于近日发表，阐述了谷歌从Borg到Kubernetes这个旅程中所获得知识和经验教训。","text":"google论文系列Borg、Omega and Kubernetes谷歌十余年从三个容器管理系统中得到的经验教训Lessons learned from three container-management systems over a decade 英文版原文链接 中文翻译参考了韩佳瑶的翻译并做了修正 &gt; 从2000年以来，谷歌基于容器研发三个容器管理系统，分别是Borg、Omega和Kubernetes。这篇论文由这三个容器集群管理系统长年开发维护的谷歌工程师Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer和John Wilkes于近日发表，阐述了谷歌从Borg到Kubernetes这个旅程中所获得知识和经验教训。 尽管对软件容器广泛传播的兴趣是最近的现象，但在谷歌我们大规模使用Linux容器已经有10多年了，而且期间我们建了三种不同的容器管理系统。Though widespread interest in software containers is a relatively recent phenomenon, at Google we have been managing Linux containers at scale for more than ten years and built three different container-management systems in that time. 每一个系统都受之前的系统影响颇深，尽管它们的诞生是出于不同原因。这篇文章描述了我们在研发和使用它们的过程中得到的经验教训。Each system was heavily influenced by its predecessors, even though they were developed for different reasons. This article describes the lessons we’ve learned from developing and operating them. 第一个在谷歌被开发出来的统一的容器管理系统，在我们内部称之为“Borg”，它管理着长时间运行的生产服务和批处理服务。这两类任务之前是由两个分离开的系统来管理的： Babysitter Global Work Queue。The first unified container-management system developed at Google was the system we internally call Borg.7 It was built to manage both long-running services and batch jobs, which had previously been handled by two separate systems: Babysitter and the Global Work Queue. The latter’s architecture 极大地影响了Borg，但却只是针对批量服务的，且两者都在Linux control groups诞生之前。The latter’s architecture strongly influenced Borg, but was focused on batch jobs; both predated Linux control groups. Borg将这两种应用所用的机器统一成一个池子，这样得以提高资源利用率，进而降低成本。Borg shares machines between these two types of applications as a way of increasing resource utilization and thereby reducing costs. 之所以可以实现这样的机器资源共享，是因为可以拿到Linux内核的容器支持（确实，Google对Linux内核的容器代码贡献了很多），这使得在对时限敏感的、且面对用户的服务和占用很多CPU资源的批处理进程提供了更好的隔离。Such sharing was possible because container support in the Linux kernel was becoming available (indeed, Google contributed much of the container code to the Linux kernel), which enabled better isolation between latency-sensitive user-facing services and CPU-hungry batch processes. 由于越来越多的应用被开发并运行在Borg上，我们的应用和底层团队开发了一个广泛的工具和服务的生态系统。这些系统提供了配置和更新job的机制，能够预测资源需求，动态地对在运行中的程序推送配置文件、服务发现、负载均衡、自动扩容、机器生命周期的管理、额度管理以及更多。As more and more applications were developed to run on top of Borg, our application and infrastructure teams developed a broad ecosystem of tools and services for it. These systems provided mechanisms for configuring and updating jobs; predicting resource requirements; dynamically pushing configuration files to running jobs; service discovery and load balancing; auto-scaling; machine-lifecycle management; quota management; and much more. 这个生态系统的发展源自谷歌内部不同团队的需求，发展的结果成为了异构的、ad-hoc系统的集合，Borg的使用者能够用几种不同的配置语言和进程来配置和沟通。The development of this ecosystem was driven by the needs of different teams inside Google, and the result was a somewhat heterogeneous, ad-hoc collection of systems that Borg’s users had to configure and interact with, using several different configuration languages and processes. 由于Borg的规模、功能的广泛性和超高的稳定性，Borg在谷歌内部依然是主要的容器管理系统。Borg remains the primary container-management system within Google because of its scale, breadth of features, and extreme robustness. Omega，作为Borg的延伸，它的出现是出于提升Borg生态系统软件工程的愿望。Omega,6 an offspring of Borg, was driven by a desire to improve the software engineering of the Borg ecosystem. Omega应用到了很多在Borg内已经被认证的成功的模式，但是是从头开始来搭建以期更为一致的构架。It applied many of the patterns that had proved successful in Borg, but was built from the ground up to have a more consistent, principled architecture. Omega存储了基于Paxos、围绕transaction的集群的状态，能够被集群的控制面板（比如调度器）接触到，使用了优化的进程控制来解决偶尔发生的冲突。Omega stored the state of the cluster in a centralized Paxos-based transaction-oriented store that was accessed by the different parts of the cluster control plane (such as schedulers), using optimistic concurrency control to handle the occasional conflicts. 这种分离允许Borgmaster的功能被区分成几个并列的组建，而不是把所有变化都放到一个单独的、巨石型的master里。This decoupling allowed the Borgmaster’s functionality to be broken into separate components that acted as peers, rather than funneling every change through a monolithic, centralized master. 许多Omega的创新（包括多个调度器）都被收录进了Borg。Many of Omega’s innovations (including multiple schedulers) have since been folded into Borg. 谷歌研发的第三个容器管理系统是Kubernetes。Kubernetes的研发和认知背景，是针对在谷歌外部的对Linux容器感兴趣的开发者以及谷歌在公有云底层商业增长的考虑。和Borg、Omega完全是谷歌内部系统相比，Kubernetes是开源的。The third container-management system developed at Google was Kubernetes.4 It was conceived of and developed in a world where external developers were becoming interested in Linux containers, and Google had developed a growing business selling public-cloud infrastructure. Kubernetes is open source—a contrast to Borg and Omega, which were developed as purely Google-internal systems. 像Omega一样，Kubernetes在其核心有一个被分享的持久存储，有组件来检测相关ojbect的变化。跟Omega不同的是，Omega把存储直接暴露给信任的控制面板的组件，而在Kubernete中，是要完全由domain-specific的提供更高一层的版本控制认证、语义、政策的REST API来接触，以服务更多的用户。Like Omega, Kubernetes has at its core a shared persistent store, with components watching for changes to relevant objects. In contrast to Omega, which exposes the store directly to trusted control-plane components, state in Kubernetes is accessed exclusively through a domain-specific REST API that applies higher-level versioning, validation, semantics, and policy, in support of a more diverse array of clients. 更重要的是，Kubernetes是由一支在集群层面应用开发能力更强的开发者开发的，他们主要的设计目标是用更容易的方法去部署和管理复杂的分布式系统，同时仍然能通过容器所提升的使用效率来受益。More importantly, Kubernetes was developed with a stronger focus on the experience of developers writing applications that run in a cluster: its main design goal is to make it easy to deploy and manage complex distributed systems, while still benefiting from the improved utilization that containers enable. 这篇文章描述了谷歌从Borg到Kubernetes这个旅程中所获得知识和经验教训。This article describes some of the knowledge gained and lessons learned during Google’s journey from Borg to Kubernetes. 容器 Containers历史上，第一个容器提供的仅仅是root file system的隔离（通过chroot），再加上FreeBSD jails提供额外的例如process ID这样的namespaces。Historically, the first containers just provided isolation of the root file system (via chroot), with FreeBSD jails extending this to additional namespaces such as process IDs. Solaris后来成为先锋并且做了很多加强的探索。Linux control groups（cgroups）运用了很多这些想法，在这个领域的发展一直延续到今天。Solaris subsequently pioneered and explored many enhancements. Linux control groups (cgroups) adopted many of these ideas, and development in this area continues today. 容器的资源隔离特性使得谷歌的资源使用率远远高出业界同行。例如，Borg使用容器将对延迟敏感、面向用户的任务和批量任务放在相通的物理机上，并会为它们预留更多的资源，这样可以解决load spikes、fail-over等问题。The resource isolation provided by containers has enabled Google to drive utilization significantly higher than industry norms. For example, Borg uses containers to co-locate batch jobs with latency-sensitive, user-facing jobs on the same physical machines. The user-facing jobs reserve more resources than they usually need—allowing them to handle load spikes and fail-over—and these mostly unused resources can be reclaimed to run batch jobs. 容器提供的资源管理工具使这些得以实现，稳定的内核层面的资源隔离也使进程之间不互相干扰。Containers provide the resource-management tools that make this possible, as well as robust kernel-level resource isolation to prevent the processes from interfering with one another. 我们通过在研发Borg的同时加强Linux容器的方式来获得成功。然而，隔离并不是完美的，容器在内核操作系统不能管理的资源隔离方面鞭长莫及，比如level 3 processor caches、内存带宽、以及容器需要被一个额外的安全层支持以抵抗云端的各种恶意攻击。We achieved this by enhancing Linux containers concurrently with Borg’s development. The isolation is not perfect, though: containers cannot prevent interference in resources that the operating-system kernel doesn’t manage, such as level 3 processor caches and memory bandwidth, and containers need to be supported by an additional security layer (such as virtual machines) to protect against the kinds of malicious actors found in the cloud. 现代的容器不仅仅是隔离机制：它也包括镜像，即包含了在容器内能够让应用跑起来的文件。A modern container is more than just an isolation mechanism: it also includes an image—the files that make up the application that runs inside the container. 在谷歌内部，MPM（Midas Package Manager）被用来建造和部署容器镜像。Within Google, MPM (Midas Package Manager) is used to build and deploy container images. 在隔离机制和MPM之间同样的共生关系，也可以在Docker daemon和Docker镜像之间被发现。The same symbiotic relationship between the isolation mechanism and MPM packages can be found between the Docker daemon and the Docker image registry. 在这篇文章剩余的篇幅中，我们会使用“容器”这个词来包含这两方面：运行时隔离和镜像。In the remainder of this article we use the word container to encompass both of these aspects: the runtime isolation and the image. 面向应用的架构（Application-oriented infrastructure）随着时间的推移，我们越来越清楚容器在更高一层使用时的好处。容器化能使数据中心从面向机器转为面向应用。这个部分讨论两个例子：Over time it became clear that the benefits of containerization go beyond merely enabling higher levels of utilization. Containerization transforms the data center from being machine oriented to being application oriented. This section discusses two examples: 容器封装了应用环境，把很多机器和操作系统的细节从应用开发者和部署底层那里抽象了出来。• Containers encapsulate the application environment, abstracting away many details of machines and operating systems from the application developer and the deployment infrastructure. 因为设计良好的容器和镜像的作用范围是一个很小的应用，因此管理容器意味着管理应用而非机器，极大简化了应用的部署和管理。• Because well-designed containers and container images are scoped to a single application, managing containers means managing applications rather than machines. This shift of management APIs from machine-oriented to application-oriented dramatically improves application deployment and introspection. 应用环境 Application EnvironmentLinux内核里的cgroup、chroot和namespace的原本是为了保护应用不受周边杂乱邻里的影响。把这些和容器镜像组合起来创建一个抽象事物把应用从运行它们的（纷杂的）操作系统里隔离出来，提高了部署可靠性，也通过减少不一致性和冲突而加快了开发速度。The original purpose of the cgroup, chroot, and namespace facilities in the kernel was to protect applications from noisy, nosey, and messy neighbors. Combining these with container images created an abstraction that also isolates applications from the (heterogeneous) operating systems on which they run. This decoupling of image and OS makes it possible to provide the same deployment environment in both development and production, which, in turn, improves deployment reliability and speeds up development by reducing inconsistencies and friction. 能让这个抽象事物得以实现的关键在于有一个自包含的镜像，它把一个应用几乎所有的依赖环境都打包然后部署在一个容器里。The key to making this abstraction work is having a hermetic container image that can encapsulate almost all of an application’s dependencies into a package that can be deployed into the container. 如果这个过程做的正确，本地的外部环境就只剩下Linux内核的system-call interface.If this is done correctly, the only local external dependencies will be on the Linux kernel system-call interface. 这个有限制的interface极大提高了镜像的便携性，它并不完美：应用仍然暴露给了OS interface，尤其是在socket选项的广泛表面上、/proc、和给ioctl call的所传参数上。While this limited interface dramatically improves the portability of images, it is not perfect: applications can still be exposed to churn in the OS interface, particularly in the wide surface area exposed by socket options, /proc, and arguments to ioctl calls. 我们希望后面类似 Open Container Initiative（OCI: https://www.opencontainers.org/）的努力能继续把容器抽象的表层能理清。Our hope is that ongoing efforts such as the Open Container Initiative (https://www.opencontainers.org/) will further clarify the surface area of the container abstraction. 然而，容器提供的隔离和对环境依赖的最低性在谷歌内部颇为有效，容器也是谷歌内部底层唯一支持的应用程序运行实体。Nonetheless, the isolation and dependency minimization provided by containers have proved quite effective at Google, and the container has become the sole runnable entity supported by the Google infrastructure. 这样的好处之一就是在任何时候，谷歌在它一整台机器上只有很少量的OS版本部署，只需要很少量的人员来管理或升级。One consequence is that Google has only a small number of OS versions deployed across its entire fleet of machines at any one time, and it needs only a small staff of people to maintain them and push out new versions. 有很多种方式可以实现这些自包含的镜像。There are many ways to achieve these hermetic images. 在Borg里，程序的二进制文件在构建时静态地连接到公司范围内repo库里已知的library版本。In Borg, program binaries are statically linked at build time to known-good library versions hosted in the companywide repository.5 即便这样，Borg容器镜像也并非100%的自包含：因为应用会共享一个所谓的基础镜像，而不是将这个基础镜像打包在每个容器中。Even so, the Borg container image is not quite as airtight as it could have been: applications share a so-called base image that is installed once on the machine rather than being packaged in each container. 这个基础镜像包含了一些容器需要用到的utilities，比如tar和libc library，因此对基础镜像的升级会影响运行中的应用，偶尔也会变成一个比较严重的问题产生来源。This base image contains utilities such as tar and the libc library, so upgrades to the base image can affect running applications and have occasionally been a significant source of trouble. 现在的容器镜像格式，比如Docker和ACI把容器进一步抽象，通过消除内在的主机OS环境依赖和要求外在的user命令来共享容器之间的镜像数据，使得距离理想的自包含性又近了一步。More modern container image formats such as Docker and ACI harden this abstraction further and get closer to the hermetic ideal by eliminating implicit host OS dependencies and requiring an explicit user command to share image data between containers. 容器作为管理的单位 Containers as the Unit of Management搭建面向容器而非机器的管理API把数据中心的关键字从机器转向了应用。Building management APIs around containers rather than machines shifts the “primary key” of the data center from machine to application. 这样有很多好处：（1）减轻应用开发者和运维团队操心机器和系统具体细节的压力；（2）提供底层团队弹性，得以升级新硬件和操作系统，但同时对在跑的应用和开发者影响甚小；（3）它把管理系统收集的telemetry（比如CPU和内存用量的metrics）和应用而非机器绑在一起，极大提升了应用监测和检查，尤其是在扩容、机器失败或者运维导致应用实例迁移的时候。This has many benefits:(1) it relieves application developers and operations teams from worrying about specific details of machines and operating systems;(2) it provides the infrastructure team flexibility to roll out new hardware and upgrade operating systems with minimal impact on running applications and their developers;(3) it ties telemetry collected by the management system (e.g., metrics such as CPU and memory usage) to applications rather than machines, which dramatically improves application monitoring and introspection, especially when scale-up, machine failures, or maintenance cause application instances to move. 容器能够注册通用的API使得管理系统和应用之间尽管互相不甚明了对方的实现细节，但也能信息流通。Containers provide convenient points to register generic APIs that enable the flow of information between the management system and an application without either knowing much about the particulars of the other’s implementation. 在Borg里，这个API是一系列HTTP终端衔接到每一个容器上。举例来说，／healthz终端对编排器报告应用的健康状态。当一个不健康的应用被发现，它就被自动终止和重启。这种自我修复对可靠的分布式系统而言是一个关键的砖头块。（Kubernetes提供了类似的功能；健康检查使用了一个用户指定的HTTP终端或者跑在容器里的exec命令。）In Borg, this API is a series of HTTP endpoints attached to each container. For example, the /healthz endpoint reports application health to the orchestrator. When an unhealthy application is detected, it is automatically terminated and restarted. This self-healing is a key building block for reliable distributed systems. (Kubernetes offers similar functionality; the health check uses a user-specified HTTP endpoint or exec command that runs inside the container.) 可以\b为容器提供或获取附加信息，并在各种用户接口显示。例如，\bBorg 应用程序可以提供动态更新的简单文本状态消息，而 Kubernetes 提供用key-value\b注解方式存储的对象的元数据中,\b可用于通讯应用结构体 。Additional information can be provided by or for containers and displayed in various user interfaces. For example, Borg applications can provide a simple text status message that can be updated dynamically, and Kubernetes provides key-value annotations stored in each object’s metadata that can be used to communicate application structure.可以通过容器本身或在管理系统中的其他参与者来设置这些注解（例如: 容器的更新版本的过程）来设置。Such annotations can be set by the container itself or other actors in the management system (e.g., the process rolling out an updated version of the container). 容器也能提供其他面向应用的监测：举例来说，Linux内核cgroups提供关于应用的资源利用数据，这些可以和先前提到的由HTTP API导出的客户metrics一起被延伸。Containers can also provide application-oriented monitoring in other ways: for example, Linux kernel cgroups provide resource-utilization data about the application, and these can be extended with custom metrics exported using HTTP APIs, as described earlier. 这些数据能够实现例如自动扩容或cAdvisor这样一般工具的开发，这些开发记录或者使用这些metrics，不需要理解每个应用的细节。因为容器就是应用，就不再需要从在一个物理机或者虚拟机上跑的多个应用来多路分配信号。This data enables the development of generic tools like an auto-scaler or cAdvisor3 that can record and use metrics without understanding the specifics of each application. Because the container is the application, there is no need to (de)multiplex signals from multiple applications running inside a physical or virtual machine. 这个更简单、更稳定一些，而且也允许对metrics和日志进行更细粒度的报告和控制。拿这个跟需要ssh到一台机器去跑top去比一下。尽管对开发者来说，ssh到他们的容器是可能的，但程序员很少会需要这么去做。This is simpler, more robust, and permits finer-grained reporting and control of metrics and logs. Compare this to having to ssh into a machine to run top. Though it is possible for developers to ssh into their containers, they rarely need to. 监测，只是一个例子。面向应用的这个变化在管理底层上是有连带效果的。我们的负载均衡器并不平衡机器间的传输，它们是针对应用实例来平衡。Monitoring is just one example. The application-oriented shift has ripple effects throughout the management infrastructure. Our load balancers don’t balance traffic across machines; they balance across application instances. 日志也是根据应用，而非机器，因此它们可以很容易的被收集以及在实例之间集合，而不受到多个应用或者操作系统的影响。我们可以查探到应用失败，更容易对这些失败的原因来归类，而不需要对它们进行机器层面信号的分离。Logs are keyed by application, not machine, so they can easily be collected and aggregated across instances without pollution from multiple applications or system operations. We can detect application failures and more readily ascribe failure causes without having to disentangle them from machine-level signals. Fundamentally, because the identity of an instance being managed by the container manager lines up exactly with the identity of the instance expected by the application developer, it is easier to build, manage, and debug applications. 最后，尽管到目前为止，我们对应用的关注和对容器的关注是1:1，但在现实中我们使用在同一台机器上联动的容器：最外面的容器提供一个资源池，里面的这些容器提供部署隔离。在Borg,最外面那层容器被称为资源调配器（或者alloc），在Kubernetes里，被称为pod。Borg也允许最顶端的应用容器跑在alloc的外面，这个带来了很多不方便，所以Kubernetes把这些规范化并且总是在一个顶端的pod里来跑应用容器，即便这个pod只有单一的一个容器。Finally, although so far we have focused on applications being 1:1 with containers, in reality we use nested containers that are co-scheduled on the same machine: the outermost one provides a pool of resources; the inner ones provide deployment isolation. In Borg, the outermost container is called a resource allocation, or alloc; in Kubernetes, it is called a pod. Borg also allows top-level application containers to run outside allocs; this has been a source of much inconvenience, so Kubernetes regularizes things and always runs an application container inside a top-level pod, even if the pod contains a single container. 一个普遍的使用样式，是用一个pod来装一个复杂应用的实例。应用的主要部分在它其中一个容器（child containers）里，其他容器跑着支持功能，例如日志处理。跟把这些功能组合到一个单一的二进制相比，这使得开发团队开发不同功能的部件容易很多，也提高了稳定性（即便主体应用有新的东西进来，日志发送依然可以继续运行）和可编辑性。A common use pattern is for a pod to hold an instance of a complex application. The major part of the application sits in one of the child containers, and other containers run supporting functions such as log rotation or click-log offloading to a distributed file system. Compared to combining the functionality into a single binary, this makes it easy to have different teams develop the distinct pieces of functionality, and it improves robustness (the offloading continues even if the main application gets wedged), composability (it’s easy to add a new small support service, because it operates in the private execution environment provided by its own container), and fine-grained resource isolation (each runs in its own resources, so the logging system can’t starve the main app, or vice versa). 编排只是开始，不是结束 Orchestration is the Beginning, Not the End原始的Borg系统可以在共享的机器上跑不同的工作负荷来提高资源利用率。但在Borg内支持服务的迅速进化显示，容器管理的本质只是开发和管理可靠的分布式系统的开始，很多不同的系统在Borg上和周边被开发，用来提高Borg所提供的基本的容器管理服务。下面这个不完整的列表给出了这些服务大概的一个范围和多样性： ● 命名和服务发现（Borg Name Service或BNS）； ● Master election（用Chubby）； ● 面向应用的负载均衡； ● 横向（实例数量）和纵向（实例尺寸）的自动扩容； ● 发布工具，用来管理新二进制和配置数据； ● 工作流程工具（例如允许跑分析多任务的pipelines在不同阶段有互相环境依赖）； ● 监测工具用来收集关于容器的信息，集合这些信息、发布到dashboard上，或者用它来激发预警。The original Borg system made it possible to run disparate workloads on shared machines to improve resource utilization. The rapid evolution of support services in the Borg ecosystem, however, showed that container management per se was just the beginning of an environment for developing and managing reliable distributed systems. Many different systems have been built in, on, and around Borg to improve upon the basic container-management services that Borg provided. The following partial list gives an idea of their range and variety: • Naming and service discovery (the Borg Name Service, or BNS).• Master election, using Chubby.2• Application-aware load balancing.• Horizontal (number of instances) and vertical (size of an instance) autoscaling.• Rollout tools that manage the careful deployment of new binaries and configuration data.• Workflow tools (e.g., to allow running multijob analysis pipelines with interdependencies between the stages).• Monitoring tools to gather information about containers, aggregate it, present it on dashboards, and use it to trigger alerts. 构建这些服务是用来解决应用开发团队所经历的问题。成功的服务被广泛采用，那其他开发者就受益。不幸的是，这些工具常常选一些怪癖的API，conventions（比如文件位置）和Borg的深度结合。一个副作用就是增加了Borg生态系统部署应用的复杂性。These services were built organically to solve problems that application teams experienced. The successful ones were picked up, adopted widely, and made other developers’ lives easier. Unfortunately, these tools typically picked idiosyncratic APIs, conventions (such as file locations), and depth of Borg integration. An undesired side effect was to increase the complexity of deploying applications in the Borg ecosystem. Kubernetes企图通过对API采用一种一致的方法来避免这种增加的复杂性。比如说，每一个Kubernetes的对象在它的描述里有三个基本的属性：对象的metadata、spec和状态（status）。 Kubernetes attempts to avert this increased complexity by adopting a consistent approach to its APIs. For example, every Kubernetes object has three basic fields in its description: Object Metadata, Specification (or Spec), and Status. 对象的metadata对系统中的所有对象都是一样的，它包含了例如对象名称、UID（特殊标示）、一个对象的版本号码（为了乐观的进程控制）以及标签（key-value对，见下面描述）。Spec和status的内容根据不同的对象类型会不同，但它们的概念是一样的：spec时用来描述对象的理想状态，而status提供了该对象目前当下的只读信息。The Object Metadata is the same for all objects in the system; it contains information such as the object’s name, UID (unique identifier), an object version number (for optimistic concurrency control), and labels (key-value pairs, see below). The contents of Spec and Status vary by object type, but their concept does not: Spec is used to describe the desired state of the object, whereas Status provides read-only information about the current state of the object. 这种统一的API带来很多好处，可以让我们更容易的了解系统，因为系统中所有对象都有类似的信息。另外，统一的API可以更容易地编写通用的工具来作用于所有对象，这样反过来也让使用者感觉更为连贯。通过对Borg和Omega的总结，Kubernetes建立在一整套可自由拆装的部件之上，可以由开发者任意延展。一个有共识的API和对象metadata结构可以使这个过程更为简单。This uniform API provides many benefits. Concretely, learning the system is simpler: similar information applies to all objects. Additionally, writing generic tools that work across all objects is simpler, which in turn enables the development of a consistent user experience. Learning from Borg and Omega, Kubernetes is built from a set of composable building blocks that can readily be extended by its users. A common API and object-metadata structure makes that much easier. For example, the pod API is usable by people, internal Kubernetes components, and external automation tools. To further this consistency, Kubernetes is being extended to enable users to add their own APIs dynamically, alongside the core Kubernetes functionality. 一致性还可以通过在Kubernetes API内解构来完成。在API组建之间考虑进行一些分离意味着在更高层的服务上需要共享一些基本的构建组件。Consistency is also achieved via decoupling in the Kubernetes API. Separation of concerns between API components means that higher-level services all share the same common basic building blocks. 一个很好的例子是在Kubernetes的RC(replication controller)和它水平自动扩容系统之间的分离。一个RC保证了对某个角色（比如“前端”）理想的pod数量的存在。A good example of this is the separation between the Kubernetes replica controller and its horizontal auto-scaling system. A replication controller ensures the existence of the desired number of pods for a given role (e.g., “front end”). 自动扩容器，反过来，需要依赖这个能力并且简单地调整理想的pod数量，不需要担心pod如何生成和删除。自动扩容器的实现能够把精力集中在需求和使用的预测，忽略如何实现这些决定的细节。The autoscaler, in turn, relies on this capability and simply adjusts the desired number of pods, without worrying about how those pods are created or deleted. The autoscaler implementation can focus on demand and usage predictions, and ignore the details of how to implement its decisions. 分离保证了多个关联但不同的组件共享一个相似的外表和感觉。举个例子，Kubernetes有三个不同的pod副本模式： ● ReplicationController ： 永远在运行的容器副本（比如web服务器）； ● DaemonSet ： 确保在集群里的每个节点上有一个单独的实例（例如日志代理）； ● Job： 一个知道如何从开始到结束运行一个（可能是并行的）批处理任务的run-to-completion的控制器。 Decoupling ensures that multiple related but different components share a similar look and feel. For example, Kubernetes has three different forms of replicated pods:• ReplicationController: run-forever replicated containers (e.g., web servers).• DaemonSet: ensure a single instance on each node in the cluster (e.g., logging agents).• Job: a run-to-completion controller that knows how to run a (possibly parallelized) batch job from start to finish. 尽管在规则上有区别，所有这三个控制器都依赖共同的pod对象来制定它们想要运行在上面的容器。Regardless of the differences in policy, all three of these controllers rely on the common pod object to specify the containers they wish to run. 一致性也可以通过不同Kubernetes组件上共同的设计形式来达到。在Borg、Omega和Kubernetes上用来提高系统弹性，有一个概念：“reconciliation controller loop”（清理控制器循环）,这个概念是来比较一个理想的状态（比如需要多少个pod才能来达到一个标签选择的query，即 label-selector query），和相对于观测到的状态（可以发现的这样的pod数量）来进行比较，然后采取行动去把这两个状态做到一致。 Consistency is also achieved through common design patterns for different Kubernetes components. The idea of a reconciliation controller loop is shared throughout Borg, Omega, and Kubernetes to improve the resiliency of a system: it compares a desired state (e.g., how many pods should match a label-selector query) against the observed state (the number of such pods that it can find), and takes actions to converge the observed and desired states. Because all action is based on observation rather than a state diagram, reconciliation loops are robust to failures and perturbations: when a controller fails or restarts it simply picks up where it left off.The design of Kubernetes as a combination of microservices and small control loops is an example of control through choreography—achieving a desired emergent behavior by combining the effects of separate, autonomous entities that collaborate. This is a conscious design choice in contrast to a centralized orchestration system, which may be easier to construct at first but tends to become brittle and rigid over time, especially in the presence of unanticipated errors or state changes. 需要避免的事情 Things to Avoid在研发这些系统的时候，我们也学到了许多关于哪些事情不该做，哪些事情值得去做的经验。我们把其中的一些写出来，期望后来者不再重蹈覆辙，而是集中精力去解决新问题。 While developing these systems we have learned almost as many things not to do as ideas that are worth doing. We present some of them here in the hopes that others can focus on making new mistakes, rather than repeating ours. 别让容器系统来管理port端口 Don’t Make the Container System Manage Port Numbers所有跑在Borg机器上的容器都共享主机的IP地址，所以Borg给容器分发了独特的port端口作为调度过程的一部分。一个容器当它移到一个新的机器上已经有时候如果在同样的机器上重启的话，会拿到一个新的端口号码。这意味着传统的例如像DNS（Domain Name System）网络服务需要被home-brew版本取代；因为服务的客户不会先验地知道给到服务的port端口，需要被告知；port端口号码不能被嵌在URL里，就需要以名字为基础的再次导向（redirection）机制；而且依赖于简单的IP地址的工具需要被重写来搞定IP：端口对的形式（port pairs）。 All containers running on a Borg machine share the host’s IP address, so Borg assigns the containers unique port numbers as part of the scheduling process. A container will get a new port number when it moves to a new machine and (sometimes) when it is restarted on the same machine. This means that traditional networking services such as the DNS (Domain Name System) have to be replaced by home-brew versions; service clients do not know the port number assigned to the service a priori and have to be told; port numbers cannot be embedded in URLs, requiring name-based redirection mechanisms; and tools that rely on simple IP addresses need to be rewritten to handle IP:port pairs. 从我们在Borg的经验来看，我们决定Kubernetes可以来给每个pod制定IP地址，这样把网络身份（即IP地址）和应用身份能统一起来。这会使得在Kubernetes上跑现成的软件容易的多：应用可以随意使用静态已知的端口（比如80作为HTTP端口），已经存在的、熟悉的工具可以被用来做网络分段、带宽调节管理。所有流行的云平台提供网络的基础层，能够有每个pod的IP，在裸机上，可以使用SDN覆盖层或者配置L3路由来管理每个机器上的多个IP.Learning from our experiences with Borg, we decided that Kubernetes would allocate an IP address per pod, thus aligning network identity (IP address) with application identity. This makes it much easier to run off-the-shelf software on Kubernetes: applications are free to use static well-known ports (e.g., 80 for HTTP traffic), and existing, familiar tools can be used for things like network segmentation, bandwidth throttling, and management. All of the popular cloud platforms provide networking underlays that enable IP-per-pod; on bare metal, one can use an SDN (Software Defined Network) overlay or configure L3 routing to handle multiple IPs per machine. 别仅仅只是给容器编号：给它们打标签 Don’t Just Number Containers: Give Them Labels如果你让用户很容易地创建容器，他们会倾向于创建很多，那么很快就会需要一种方式来管理和组织它们。Borg对于群组的相同的task提供了jobs（对于容器而言任务的名称）。一个job是一个压缩的容器（vector）装了一个或多个相同的task，从0开始计数。这提供了许多能量，而且很简单直白，但时间长了我们又会后悔它过于死板。比如说，当一个task死掉了，需要在另一台机器上被重新启动，在task这个vector上的相同的slot就要双倍的工作：既要指认这个新的备份，同时还要指向旧的那个，万一可能需要做debug。当task出现在vector的当中，那vector就有洞。因此vector很难去支持在Borg的一层上跨越多个集群的job.同时，也有潜在的、不期而遇的在Borg更新job的语意上（典型的是在做滚动升级的时候按照index标记来重启）和应用使用的task index标记(比如做一些sharding活着跨task的数据的分区)的互动：如果应用使用基于task index的range sharding，那么Borg的重启政策会导致拿不到数据，因为它会拉掉附近的任务。Borg也没有简单的办法去job里面增加跟应用有关的metadata，比如角色属性（比如“前端”）或者展示的状态（比如是canary），所以人们要把这些信息写到job名称里，这样他们可以用常规表达式（regular expression）来解析。If you allow users to create containers easily, they tend to create lots of them, and soon need a way to group and organize them. Borg provides jobs to group identical tasks (its name for containers). A job is a compact vector of one or more identical tasks, indexed sequentially from zero. This provides a lot of power and is simple and straightforward, but we came to regret its rigidity over time. For example, when a task dies and has to be restarted on another machine, the same slot in the task vector has to do double duty: to identify the new copy and to point to the old one in case it needs to be debugged. When tasks in the middle of the vector exit, the vector ends up with holes. The vector makes it very hard to support jobs that span multiple clusters in a layer above Borg. There are also insidious, unexpected interactions between Borg’s job-update semantics (which typically restarts tasks in index order when doing rolling upgrades) and an application’s use of the task index (e.g., to do sharding or partitioning of a dataset across the tasks): if the application uses range sharding based on the task index, Borg’s restart policy can cause data unavailability, as it takes down adjacent tasks. Borg also provides no easy way to add application-relevant metadata to a job, such as role (e.g., “frontend”), or rollout status (e.g., “canary”), so people encode this information into job names that they decode using regular expressions. 相比之下，Kubernetes主要使用标签（labels）来识别成组的容器。一个标签是一对key/value组，包含着容器信息可以用来识别对象。一个pod可能有这样的标签：role=frontend 和 stage=production，表明这个容器服务于前端生产。标签可以动态地被自动工具、用户来添加、移除和修改，也可以被其他不同的团队独立地来管理他们自己的标签。成组的对象，可以由label selectors来定义（比如 stage==production &amp;&amp; role==frontend）。这些组（set）可以重叠，而且一个对象可以在多个的组（set）里，因此标签本身要比明确的对象列表或简单静态的属性更灵活。因为一个组（set）是由一个动态队列来定义的，一个新的组可以在任何时候被生成。在Kubernetes里label selectors是grouping（成组）的机制，来定义跨越多个实体的管理操作的范围。 即便在那样的环境里知道在一个set里的一个task的身份是很有帮助的（比如说静态角色的分配和工作分区或分片），适当的每个pod有标签可以被用来再次产生任务标示的效果，尽管这个是应用的责任（或者一些其他在Kubernetes外部的管理系统的责任）来提供这样的标签。标签和标签选择器提供了一个对这两者的最好的通用机制。 In contrast, Kubernetes primarily uses labels to identify groups of containers. A label is a key/value pair that contains information that helps identify the object. A pod might have the labels role=frontend and stage=production, indicating that this container is serving as a production front-end instance. Labels can be dynamically added, removed, and modified by either automated tools or users, and different teams can manage their own labels largely independently. Sets of objects are defined by label selectors (e.g., stage==production &amp;&amp; role==frontend). Sets can overlap, and an object can be in multiple sets, so labels are inherently more flexible than explicit lists of objects or simple static properties. Because a set is defined by a dynamic query, a new one can be created at any time. Label selectors are the grouping mechanism in Kubernetes, and define the scope of all management operations that can span multiple entities. Even in those circumstances where knowing the identity of a task in a set is helpful (e.g., for static role assignment and work-partitioning or sharding), appropriate per-pod labels can be used to reproduce the effect of task indexes, though it is the responsibility of the application (or some other management system external to Kubernetes) to provide such labeling. Labels and label selectors provide a general mechanism that gives the best of both worlds. 对所有权要谨慎 Be Careful with Ownership在Borg里，tasks并不是独立于jobs存在的。生成一个job，也会生成它的task，那些tasks永远和特定的job是有关联的，如果删除job，也会删除task。这样很方便，但也会有一个主要的缺点：因为只有一个成组的机制，需要来解决所有出现的情况。举例来说，一个job需要存储参数，这些参数或者是对应服务（service）或者是对应批量工作（batch job）但不会是两者同时，而且用户必须要写出workarounds当job的抽象无法来解决某个情况的时候（比如一个DaemonSet对这个集群里的所有节点都去复制一个简单的pod）。 In Borg, tasks do not exist independently from jobs. Creating a job creates its tasks; those tasks are forever associated with that particular job, and deleting the job deletes the tasks. This is convenient, but it has a major drawback: because there is only one grouping mechanism, it needs to handle all use cases. For example, a job has to store parameters that make sense only for service or batch jobs but not both, and users must develop workarounds when the job abstraction doesn’t handle a use case (e.g., a DaemonSet that replicates a single pod to all nodes in the cluster). 在Kubernetes里，pod生命周期的管理组件例如RC决定了哪个pod它们有责任要用标签选择器，因此多个控制器都可能会认为它们自己对于一个单一的pod有管辖权。通过适当的配置选择来预防这样的冲突就非常重要。但是标签的弹性也有额外的好处，比如说，控制器和pod的分离意味着可以孤立和启用一些容器。考虑到一个负载均衡的服务使用一个标签选择器去标示一组pod去发送请求。如果这些pod中的一个行为异常，那这个pod的被Kubernetes服务负载均衡器识别出来的标签会被删除、这个pod被隔离不再进行服务。这个pod不再服务接受请求，但它会保持线上的状态，在原处可以被debug.同时，管理pod的RC自动实现服务，为有问题的pod创建一个复制的pod. In Kubernetes, pod-lifecycle management components such as replication controllers determine which pods they are responsible for using label selectors, so multiple controllers might think they have jurisdiction over a single pod. It is important to prevent such conflicts through appropriate configuration choices. But the flexibility of labels has compensating advantages—for example, the separation of controllers and pods means that it is possible to “orphan” and “adopt” containers. Consider a load-balanced service that uses a label selector to identify the set of pods to send traffic to. If one of these pods starts misbehaving, that pod can be quarantined from serving requests by removing one or more of the labels that cause it to be targeted by the Kubernetes service load balancer. The pod is no longer serving traffic, but it will remain up and can be debugged in situ. In the meantime, the replication controller managing the pods that implements the service automatically creates a replacement pod for the misbehaving one. 不要暴露raw state Don’t Expose Raw StateBorg、Omega和Kubernetes之间一个关键的差别在于它们的API构架。Borgmaster是一个单一的组件，它知道每一个API运作的语义。它包含了诸如关于jobs,tasks和机器的状态机器的集群管理的逻辑；它跑基于Paxos的复制存储系统用来记录master的状态。反观Omega，Omega除了存储之外没有集中的部件，存储也是简单地汇集了被动的状态信息以及加强乐观的并行进程控制：所有的逻辑和语义都被推进存储的client里，直接读写存储的内容。在实践中，每一个Omega的部件为了存储使用同样的客户端library，来打包或者解体数据结构、重新尝试活着加强语义的一致性。A key difference between Borg, Omega, and Kubernetes is in their API architectures. The Borgmaster is a monolithic component that knows the semantics of every API operation. It contains the cluster management logic such as the state machines for jobs, tasks, and machines; and it runs the Paxos-based replicated storage system used to record the master’s state. In contrast, Omega has no centralized component except the store, which simply holds passive state information and enforces optimistic concurrency control: all logic and semantics are pushed into the clients of the store, which directly read and write the store contents. In practice, every Omega component uses the same client-side library for the store, which does packing/unpacking of data structures, retries, and enforces semantic consistency. Kubernetes选择了一个中间地提供了像Omega部件结构的弹性和可扩容性，同时还能加强系统层面的无变化、政策和数据传输。它通过强制所有存储接触必须通过一个中央的API服务器来隐藏存储的实现细节和给对象验证、版本控制提供服务来做到这些。在Omega里，client的部件互相之间是分离的，可以进化或者单独被替换（这对开源环境而言尤其重要），但中央化对加强共同语义、不变性和政策会容易很多。 Kubernetes picks a middle ground that provides the flexibility and scalability of Omega’s componentized architecture while enforcing system-wide invariants, policies, and data transformations. It does this by forcing all store accesses through a centralized API server that hides the details of the store implementation and provides services for object validation, defaulting, and versioning. As in Omega, the client components are decoupled from one another and can evolve or be replaced independently (which is especially important in the open-source environment), but the centralization makes it easy to enforce common semantics, invariants, and policies. 一些开放性的难题 Some Open, Hard Problems有了八年的容器管理经验，我们感觉依然还有大量的问题我们没有很好的解决方案。这个部分描述了一些我们感到特别棘手的问题，作为抛砖引玉吧。 Even with years of container-management experience, we feel there are a number of problems that we still don’t have good answers for. This section describes a couple of particularly knotty ones, in the hope of fostering discussion and solutions. Configuration配置在所有我们面对的问题中，最多的心思和笔墨涉及到的都是关于管理配置，即一整套的提供给应用的配置，而非硬生生写进应用里去。我们完全可以把整篇文章都拿来写这个主题（可能都说不完）。下面这些是一些我们想要强调的内容。 首先，应用配置变成了一个关联一切的抓手，来实现所有的东西，所有这些容器管理系统（尚且）不做的事情，包括： ● 样板化简约（比如把tast重启的政策调整到相适应的负载工作量，例如服务或者批处理工作）； ● 调整和验证应用参数以及command-line参数； ● 实现例如打包镜像管理的缺失API抽象的替代解决方法； ● 应用配置模版的library； ● 发布管理工具； ● 镜像版本说明。 Of all the problems we have confronted, the ones over which the most brainpower, ink, and code have been spilled are related to managing configurations—the set of values supplied to applications, rather than hard-coded into them. In truth, we could have devoted this entire article to the subject and still have had more to say. What follows are a few highlights. First, application configuration becomes the catch-all location for implementing all of the things that the container-management system doesn’t (yet) do. Over the history of Borg this has included: • Boilerplate reduction (e.g., defaulting task-restart policies appropriate to the workload, such as service or batch jobs). • Adjusting and validating application parameters and command-line flags. • Implementing workarounds for missing API abstractions such as package (image) management. • Libraries of configuration templates for applications. • Release-management tools. • Image version specification. 为了解决这些要求、配置管理系统趋向于发明一个domain-specific的配置语言，最终具有图灵完备性，起源于希望能够在配置的数据里进行计算（比如对一个服务调整给它的内存，作为在一个服务里进行分区的功能）。结果就产生一个难以理解的“配置是代码”，大家都通过不在应用当中hardcode参数来尽量避免的这种情况。它并没有减少操作上的复杂性或者使得配置更容易debug或改变，它只是把计算从一个真正的编程语言挪到了一个特殊领域。 我们相信最有效的方法是去接受这个需求，拥抱无所不在的程序配置和在计算和数据之间保持一个清楚的界线。代表数据的语言应该是简单的、仅数据格式的，比如像JSON或者YAML，对这种数据的程序化修改应该在一个真实的编程语言里，有被很好理解的语义和工具。有趣的是，同样的在计算和数据之间的分离在前端开发的不同领域是雷同的，比如像Angular在markup（数据）和JavaScript（计算）之间是有清晰的划分的。To cope with these kinds of requirements, configuration-management systems tend to invent a domain-specific configuration language that (eventually) becomes Turing complete, starting from the desire to perform computation on the data in the configuration (e.g., to adjust the amount of memory to give a server as a function of the number of shards in the service). The result is the kind of inscrutable “configuration is code” that people were trying to avoid by eliminating hard-coded parameters in the application’s source code. It doesn’t reduce operational complexity or make the configurations easier to debug or change; it just moves the computations from a real programming language to a domain-specific one, which typically has weaker development tools (e.g., debuggers, unit test frameworks, etc). We believe the most effective approach is to accept this need, embrace the inevitability of programmatic configuration, and maintain a clean separation between computation and data. The language to represent the data should be a simple, data-only format such as JSON or YAML, and programmatic modification of this data should be done in a real programming language, where there are well-understood semantics, as well as good tooling. Interestingly, this same separation of computation and data can be seen in the disparate field of front-end development with frameworks such as Angular that maintain a crisp separation between the worlds of markup (data) and JavaScript (computation). 依赖条件的管理 Dependency Management起一个服务往往也意味着提供一系列相关的服务（监控、存储、CI/CD等等）。如果一个应用对其他应用有依赖，其他这些依赖条件（和任何它们可能有涉及的依赖条件）能够被集群系统自动管理，是不是很好呢？ 更麻烦的是，对依赖条件的实例化很少会像起一个新的备份这么简单，比如说，它可能会需要对现有的服务注册一个新的消费者（比如Bigtable as a service）然后通过这些间接的依赖环境来传递认证、授权以及账号信息。然而，基本上没有系统会抓、保持或者透露这些依赖信息，所以在底层自动化这些即便是非常常见的情况都是近乎不可能的。起来一个新的应用对用户来说就很复杂，对开发者而言来建新的服务就变难，经常导致一些最佳实践无法进行，影响服务的可靠性。 Standing up a service typically also means standing up a series of related services (monitoring, storage, CI/CD, etc). If an application has dependencies on other applications, wouldn’t it be nice if those dependencies (and any transitive dependencies they may have) were automatically instantiated by the cluster-management system? To complicate things, instantiating the dependencies is rarely as simple as just starting a new copy—for example, it may require registering as a consumer of an existing service (e.g., Bigtable as a service) and passing authentication, authorization, and billing information across those transitive dependencies. Almost no system, however, captures, maintains, or exposes this kind of dependency information, so automating even common cases at the infrastructure level is nearly impossible. Turning up a new application remains complicated for the user, making it harder for developers to build new services, and often results in the most recent best practices not being followed, which affects the reliability of the resulting service. 一个标准的问题是：如果是手动更新，很难保持依赖信息的及时更新。而且同时，能自动地（比如跟踪access）决定它的这种企图也无法掌握需要了解结果的语义信息。（比如是否这个acess要给那个实例？或者任何一个实例就足够了？）一个能够改进的可能是要求应用枚举它所依赖的服务，然后让底层拒绝对其他服务的接触（我们在我们的build system里对compiler imports这么做过）。这个动机是让底层做有用的事情，比如自动的setup、认证和连接。 不幸的是，我们所观察到的系统在表达、分析和使用系统依赖这方面的复杂性都太高，所以它们还没有被夹到一个主流的容器管理系统里。我们依然希望Kubernetes可能可以成为一个这样的平台，在这个平台上有这样的工具，但这么做是一个很大的挑战。A standard problem is that it is hard to keep dependency information up to date if it is provided manually, and at the same time attempts to determine it automatically (e.g., by tracing accesses) fail to capture the semantic information needed to understand the result. (Did that access have to go to that instance, or would any instance have sufficed?) One possible way to make progress is to require that an application enumerate the services on which it depends, and have the infrastructure refuse to allow access to any others. (We do this for compiler imports in our build system.1) The incentive would be enabling the infrastructure to do useful things in return, such as automatic setup, authentication, and connectivity. Unfortunately, the perceived complexity of systems that express, analyze, and use system dependencies has been too high, and so they haven’t yet been added to a mainstream container-management system. We still hope that Kubernetes might be a platform on which such tools can be built, but doing so remains an open challenge. 结语 Conclusions十多年搭建容器管理系统的经验教会了我们很多。而且我们把很多已有的经验融入进了Kubernetes，谷歌最近的这个容器管理系统。它的目标是基于容器的能力来提供编程生产力方面的极大收获，简化人工和自动化系统管理。我们希望你会来加入我们来延伸和提高这个项目。 A decade’s worth of experience building container-management systems has taught us much, and we have embedded many of those lessons into Kubernetes, Google’s most recent container-management system. Its goals are to build on the capabilities of containers to provide significant gains in programmer productivity and ease of both manual and automated system management. We hope you’ll join us in extending and improving it. References Bazel: {fast, correct}—choose two; http://bazel.io. Burrows, M. 2006. The Chubby lock service for loosely coupled distributed systems. Symposium on Operating System Design and Implementation (OSDI), Seattle, WA. cAdvisor; https://github.com/google/cadvisor. Kubernetes; http://kubernetes.io/. Metz, C. 2015. Google is 2 billion lines of code—and it’s all in one place. Wired (September); http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/. Schwarzkopf, M., Konwinski, A., Abd-el-Malek, M., Wilkes, J. 2013. Omega: flexible, scalable schedulers for large compute clusters. European Conference on Computer Systems (EuroSys), Prague, Czech Republic. Verma, A., Pedrosa, L., Korupolu, M. R., Oppenheimer, D., Tune, E., Wilkes, J. 2015. Large-scale cluster management at Google with Borg. European Conference on Computer Systems (EuroSys), Bordeaux, France. Brendan Burns (@brendandburns) is a software engineer at Google, where he co-founded the Kubernetes project. He received his Ph.D. from the University of Massachusetts Amherst in 2007. Prior to working on Kubernetes and cloud, he worked on low-latency indexing for Google’s web-search infrastructure. Brian Grant is a software engineer at Google. He was previously a technical lead of Borg and founder of the Omega project and is now design lead of Kubernetes. David Oppenheimer is a software engineer at Google and a tech lead on the Kubernetes project. He received a PhD from UC Berkeley in 2005 and joined Google in 2007, where he was a tech lead on the Borg and Omega cluster-management systems prior to Kubernetes. Eric Brewer is VP Infrastructure at Google and a professor at UC Berkeley, where he pioneered scalable servers and elastic infrastructure. John Wilkes has been working on cluster management and infrastructure services at Google since 2008. Before that, he spent time at HP Labs, becoming an HP and ACM Fellow in 2002. He is interested in far too many aspects of distributed systems, but a recurring theme has been technologies that allow systems to manage themselves. In his spare time he continues, stubbornly, trying to learn how to blow glass. Copyright © 2016 by the ACM. All rights reserved. acmqueue Originally published in Queue vol. 14, no. 1—see this item in the ACM Digital Library","categories":[{"name":"Distributed-Computer-Systems","slug":"Distributed-Computer-Systems","permalink":"http://hikings.github.io/categories/Distributed-Computer-Systems/"}],"tags":[{"name":"DS","slug":"DS","permalink":"http://hikings.github.io/tags/DS/"},{"name":"PAPER","slug":"PAPER","permalink":"http://hikings.github.io/tags/PAPER/"},{"name":"Containers","slug":"Containers","permalink":"http://hikings.github.io/tags/Containers/"}]},{"title":"Install tensorflow on centos","slug":"2016-10-15-Install_tensorflow_centos","date":"2016-10-07T04:01:33.000Z","updated":"2016-10-15T15:45:37.000Z","comments":true,"path":"2016/10/07/2016-10-15-Install_tensorflow_centos/","link":"","permalink":"http://hikings.github.io/2016/10/07/2016-10-15-Install_tensorflow_centos/","excerpt":"[root@localhost ~]# pip listCython (0.24.1)funcsigs (1.0.2)mock (2.0.0)numpy (1.11.2)pandas (0.19.0)pbr (1.10.0)pip (8.1.2)protobuf (3.0.0)python-dateutil (2.5.3)pytz (2016.7)setuptools (28.4.0)six (1.10.0)tensorflow (0.11.0rc0)wheel (0.29.0)[root@localhost ~]# ls","text":"[root@localhost ~]# pip listCython (0.24.1)funcsigs (1.0.2)mock (2.0.0)numpy (1.11.2)pandas (0.19.0)pbr (1.10.0)pip (8.1.2)protobuf (3.0.0)python-dateutil (2.5.3)pytz (2016.7)setuptools (28.4.0)six (1.10.0)tensorflow (0.11.0rc0)wheel (0.29.0)[root@localhost ~]# ls https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html Ubuntu/Linux 64-bit, CPU only, Python 2.7pip install –upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl Successfully installed setuptools-28.5.0 tensorflow-0.11.0rc0 [root@localhost part2]# python wide.pyTraceback (most recent call last): File “wide.py”, line 2, in import tensorflow as tf File “/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/init.py”, line 23, in from tensorflow.python import * File “/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/init.py”, line 49, in from tensorflow.python import pywrap_tensorflow File “/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py”, line 28, in _pywrap_tensorflow = swig_import_helper() File “/usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py”, line 24, in swig_import_helper _mod = imp.load_module(‘_pywrap_tensorflow’, fp, pathname, description)ImportError: /lib64/libc.so.6: version GLIBC_2.14&#39; not found (required by /usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so) 1.试图运行程序，提示&quot;libc.so.6: versionGLIBC_2.14’ not found”,原因是系统的glibc版本太低，软件编译时使用了较高版本的glibc引起的： 2.查看系统glibc支持的版本：[cpp] view plain copy[ghui@StuOS bin]$ strings /lib64/libc.so.6 |grep GLIBC_GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_PRIVATE [root@localhost part2]# rpm -qa |grep glibcglibc-2.12-1.192.el6.i686glibc-2.12-1.192.el6.x86_64vzdummy-glibc-2.12-1.7.el6.noarchglibc-devel-2.12-1.192.el6.x86_64glibc-common-2.12-1.192.el6.x86_64glibc-headers-2.12-1.192.el6.x86_64 3.可以看到最高只支持2.12版本，所以考虑编译解决这个问题： a. 到http://www.gnu.org/software/libc/下载最新版本，我这里下载了glibc-2.14.tar.xz 这个版本，解压到任意目录准备编译 b.这里解压到/var/VMdisks/glibc-2.14/ c.在glibc源码目录建立构建目录，并cd进入构建目录[cpp] view plain copy[ghui@StuOS glibc-2.14]$ mkdir build[cpp] view plain copy[ghui@StuOS glibc-2.14]$ cd build d.运行configure配置，make &amp;&amp; sudo make install [cpp] view plain copy[ghui@StuOS build]$ ../configure –prefix=/opt/glibc-2.14[ghui@StuOS build]$ make -j4[ghui@StuOS build]$ sudo make install[sudo] password for ghui: 4.临时修改环境变量[cpp] view plain copy[ghui@StuOS bin]$ export LD_LIBRARY_PATH=/opt/glibc-2.14/lib:$LD_LIBRARY_PATH 在CentOS 6环境下升级Android SDK的platform-tools版本到最新的23时，报/lib64/libc.so.6: version `GLIBC_2.14’ not found错误，升级glibc2.14出现glibc-2.14/etc/ld.so.conf: No such file or directory错误，使用命令touch /glibc-2.14/etc/ld.so.conf不再报错，但查看glibc安装版本还是不对，系统的glibc版本始终是glibc-2.12，也就是依然没有安装进去，使用export LD_LIBRARY_PATH=/opt/glibc-2.14/lib:$LD_LIBRARY_PATH修改过临时变量，都不能解决cp -r /etc/ld.so.c* /opt/glibc-2.14/etc/ln -sf /opt/glibc-2.14/lib/libc-2.14.so /lib64/libc.so.6 You cannot update glibc on Centos 6 safely. However you can install 2.14 alongside 2.12 easily, then use it to compile projects etc. Here is how: mkdir ~/glibc_install; cd ~/glibc_installwget http://ftp.gnu.org/gnu/glibc/glibc-2.14.tar.gztar zxvf glibc-2.14.tar.gzcd glibc-2.14mkdir buildcd build../configure –prefix=/opt/glibc-2.14make &amp;&amp; make installexport LD_LIBRARY_PATH=/opt/glibc-2.14/lib cd glibc-2.17mkdir buildcd build../configure –prefix=/opt/glibc-2.17make &amp;&amp; make installexport LD_LIBRARY_PATH=/opt/glibc-2.17/lib diJHMKULUQKY","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://hikings.github.io/categories/MachineLearning/"}],"tags":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://hikings.github.io/tags/MachineLearning/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://hikings.github.io/tags/Tensorflow/"},{"name":"Centos","slug":"Centos","permalink":"http://hikings.github.io/tags/Centos/"}]},{"title":"使用并发在2s完成10G文件的校验","slug":"2016-05-21-2s-walk-100k-files","date":"2016-05-21T13:27:36.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/05/21/2016-05-21-2s-walk-100k-files/","link":"","permalink":"http://hikings.github.io/2016/05/21/2016-05-21-2s-walk-100k-files/","excerpt":"AKKA使用了SBT，相比gradle更简洁，maven就不用来比较了。 但是受网络的影响，从仓库下载akka和scale的库不是件容易的事情，前后折腾了几个小时 构建的时候遇到个文件，明明jar文件存在，但是编译报错 意识到jar文件可能没有下载完整 手工检查是不可能的","text":"AKKA使用了SBT，相比gradle更简洁，maven就不用来比较了。 但是受网络的影响，从仓库下载akka和scale的库不是件容易的事情，前后折腾了几个小时 构建的时候遇到个文件，明明jar文件存在，但是编译报错 意识到jar文件可能没有下载完整 手工检查是不可能的 写了个小程序还帮忙，分别写了两个实现 一个使用 java nio 库实现并发的目录遍历和文件检测 一个使用 go 的 channel 来实现并发的处理 结果是（环境检测了.m2/.ivy/.gradle, size共超过10G） java nio 4秒完成检测 go channel 版本 2秒完成检测 go filepath.walk 版本 10秒完成检测 结论：go channel 的并发让人惊叹，java nio不可小觑","categories":[{"name":"Go","slug":"Go","permalink":"http://hikings.github.io/categories/Go/"}],"tags":[{"name":"GoLang","slug":"GoLang","permalink":"http://hikings.github.io/tags/GoLang/"},{"name":"Concurrent","slug":"Concurrent","permalink":"http://hikings.github.io/tags/Concurrent/"},{"name":"Parallelism","slug":"Parallelism","permalink":"http://hikings.github.io/tags/Parallelism/"},{"name":"NIO","slug":"NIO","permalink":"http://hikings.github.io/tags/NIO/"}]},{"title":"Go实现的简单爬虫","slug":"2016-05-15-a-sample-crwal-by-go","date":"2016-05-15T05:27:36.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/05/15/2016-05-15-a-sample-crwal-by-go/","link":"","permalink":"http://hikings.github.io/2016/05/15/2016-05-15-a-sample-crwal-by-go/","excerpt":"这段时间整理分布式经典论文, 这个自己手工来下载是个很吃力的事情 如果还想要进行一些处理方便使用,比如按引用排序,按年代排序,按作者来分类,这时候写个几十行的小爬虫就很方便了. 一般下载资料,获取峰会材料等等的时候都会用爬虫来处理 过去都是用 python 爬虫, 主要使用 BeautifulSoup 来解析, 用 requests 来请求 现在想试试用 go 处理,今天刚好发挥了作用 因为golang.org被墙, 很多库是没有办法下载的例如 https://godoc.org/golang.org/x/net 没办法 go get golang.org/x/net 了。可以用 go get github.com/golang/net 替代 但是很多库,手工处理太坑了, 写了个小爬虫, 生成了个批处理文件 在 golang.org/x 目录里面执行就获得了素有的库. 批处理","text":"这段时间整理分布式经典论文, 这个自己手工来下载是个很吃力的事情 如果还想要进行一些处理方便使用,比如按引用排序,按年代排序,按作者来分类,这时候写个几十行的小爬虫就很方便了. 一般下载资料,获取峰会材料等等的时候都会用爬虫来处理 过去都是用 python 爬虫, 主要使用 BeautifulSoup 来解析, 用 requests 来请求 现在想试试用 go 处理,今天刚好发挥了作用 因为golang.org被墙, 很多库是没有办法下载的例如 https://godoc.org/golang.org/x/net 没办法 go get golang.org/x/net 了。可以用 go get github.com/golang/net 替代 但是很多库,手工处理太坑了, 写了个小爬虫, 生成了个批处理文件 在 golang.org/x 目录里面执行就获得了素有的库. 批处理 方法1 利用 goquery 和 gocrawlhttps://github.com/PuerkitoBio/goqueryhttps://github.com/PuerkitoBio/gocrawl 方法2 利用 go_spiderhttps://github.com/hu17889/go_spider 12345spider.NewSpider(NewMyPageProcesser(), \"TaskName\"). AddUrl(\"https://github.com/hu17889?tab=repositories\", \"html\"). // Start url, html is the responce type (\"html\" or \"json\") AddPipeline(pipeline.NewPipelineConsole()). // Print result on screen SetThreadnum(3). // Crawl request by three Coroutines Run() 方法3 利用 pholcushttps://github.com/henrylee2cn/pholcus是个带界面的框架,比较重,不考虑了,发在这里供参考借鉴 方法4 不使用库, 不推荐https://gorbledlog.appspot.com/article?id=e771244bd8db5838e8df6b3a09db14a7只引用了 “fmt” , 没有办法理解","categories":[{"name":"Go","slug":"Go","permalink":"http://hikings.github.io/categories/Go/"}],"tags":[{"name":"GoLang","slug":"GoLang","permalink":"http://hikings.github.io/tags/GoLang/"},{"name":"Crawler","slug":"Crawler","permalink":"http://hikings.github.io/tags/Crawler/"}]},{"title":"免费的跨平台多语言开发工具 VSCode","slug":"2016-05-14-cross-platform-IDE","date":"2016-05-14T15:27:36.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/05/14/2016-05-14-cross-platform-IDE/","link":"","permalink":"http://hikings.github.io/2016/05/14/2016-05-14-cross-platform-IDE/","excerpt":"多年来一直在多个平台上用多种语言开发系统。从大型系统架构角度这是必然也是很好的选择，用擅长的语言做对应的事，人员的技能门槛也不会成为制约。 一直希望一个能够跨 unix/linux/windows 三个平台的开发工具，当然vim是可以的","text":"多年来一直在多个平台上用多种语言开发系统。从大型系统架构角度这是必然也是很好的选择，用擅长的语言做对应的事，人员的技能门槛也不会成为制约。 一直希望一个能够跨 unix/linux/windows 三个平台的开发工具，当然vim是可以的 不过这是多么悲催的人才需要学习这么多系统啊 OS 覆盖了 UNIX : Solaris Linux : Suse, Ubuntu, Debian, Centos Windows DB 覆盖了 Oracle, DB2, Sybase, Mysql, PostgreSQL Oracle, SybaseIQ Redis, Mongodb, leveldb, etcd 还有一堆数不清的内存数据库 语言覆盖了 Cpp, Java, Python, Js, Node, Html 现在又多了 Go 和 Scala 当前使用的工具主要是 eclipse -&gt; intelljidea vim sublime Atom 在跨平台上现在也是个不错的选择,但是还是不够流畅 总是有一些不如意 比如对json的支持 比如java的开发工具 eclipse/ inteljidea 对内存的占用了迟滞的响应速度 从这段时间开发Go来看， VSCode 是个不错的选择 内存占用相对低（不过启动了多个实例）, intelljidea 700M 响应快 无需外部工具的diff 编辑大文件快 语言支持齐全 免费用于Windows、Mac OS X和Linux 并且支持扩展，包括Go/Python/Debugger for Chrome的支持, 并支持类似sublime的ext管理 附上链接:https://code.visualstudio.com/ vscode-go pluginhttps://github.com/Microsoft/vscode-go User Settings，找到.vscode/settings.json).1234567891011121314151617// Specifies the GOPATH to use when no environment variable is set.&#123; &quot;go.buildOnSave&quot;: true, &quot;go.lintOnSave&quot;: true, &quot;go.vetOnSave&quot;: true, &quot;go.buildTags&quot;: &quot;&quot;, &quot;go.buildFlags&quot;: [], &quot;go.lintFlags&quot;: [], &quot;go.vetFlags&quot;: [], &quot;go.coverOnSave&quot;: false, &quot;go.useCodeSnippetsOnFunctionSuggest&quot;: false, &quot;go.formatOnSave&quot;: true, &quot;go.formatTool&quot;: &quot;goreturns&quot;, &quot;go.goroot&quot;: &quot;/usr/local/go&quot;, &quot;go.gopath&quot;: &quot;/Users/lukeh/go&quot;, &quot;go.gocodeAutoBuild&quot;: false&#125; gocode: go get -u -v github.com/nsf/gocode godef: go get -u -v github.com/rogpeppe/godef golint: go get -u -v github.com/golang/lint/golint go-outline: go get -u -v github.com/lukehoban/go-outline goreturns: go get -u -v sourcegraph.com/sqs/goreturns gorename: go get -u -v golang.org/x/tools/cmd/gorename gopkgs: go get -u -v github.com/tpng/gopkgs go-symbols: go get -u -v github.com/newhook/go-symbols guru: go get -u -v golang.org/x/tools/cmd/guru 123456789go get -u -v github.com/nsf/gocodego get -u -v github.com/rogpeppe/godefgo get -u -v github.com/golang/lint/golintgo get -u -v github.com/lukehoban/go-outlinego get -u -v sourcegraph.com/sqs/goreturnsgo get -u -v golang.org/x/tools/cmd/gorenamego get -u -v github.com/tpng/gopkgsgo get -u -v github.com/newhook/go-symbolsgo get -u -v golang.org/x/tools/cmd/guru","categories":[{"name":"IDE","slug":"IDE","permalink":"http://hikings.github.io/categories/IDE/"}],"tags":[{"name":"IDE","slug":"IDE","permalink":"http://hikings.github.io/tags/IDE/"}]},{"title":"并发编程的春天","slug":"2016-05-07-concurrent-and-parallelism","date":"2016-05-07T15:19:32.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/05/07/2016-05-07-concurrent-and-parallelism/","link":"","permalink":"http://hikings.github.io/2016/05/07/2016-05-07-concurrent-and-parallelism/","excerpt":"在过去的30年里，计算机的性能是在摩尔定律的推动下，从现在开始，这将由Amdahl’s Law决定。编写代码，有效地利用多个处理器可以是非常具有挑战性的。” -Doron Rajwan Amdahl’s Law：每个程序都分为串行与并行部分，降低串行的比重，可提高程序的效率。 现在系统面临下面的挑战： 单节点上如何充分的利用多核 云化或分布式网络化环境下越来越复杂的编程问题 负责的部署和一致性检测难度","text":"在过去的30年里，计算机的性能是在摩尔定律的推动下，从现在开始，这将由Amdahl’s Law决定。编写代码，有效地利用多个处理器可以是非常具有挑战性的。” -Doron Rajwan Amdahl’s Law：每个程序都分为串行与并行部分，降低串行的比重，可提高程序的效率。 现在系统面临下面的挑战： 单节点上如何充分的利用多核 云化或分布式网络化环境下越来越复杂的编程问题 负责的部署和一致性检测难度 并行并不是新事物 SUN 的 SPARC 并行和可靠性一直优于 x86，但是输给了成本。（现在uinx是不是都转到linux上了？） x86 直到 AMD 出来搅局，才逐步转向多核，但是很明显，桌面应用受限于程序员的水平，仍然不能重复利用多核的能力 GPU 提供指令级别的并行能力，挖掘比特币中大放异彩 就传统利用线程、进程模型的应用或者服务没有办法利用多核的优势这个问题，现在发现有一些新思路 scale spark , akka 的出色表现，特点是 actor 模型，在 Apache 的 Flime 项目中也选择的 akka google 推出 go, 这个面向工程的语言引入了CSP编程模型，Go语言的动态栈使得轻量级线程goroutine的初始栈可以很小，因此创建一个goroutine的代价很小，创建百万级的goroutine完全是可行的。 Java 1.5 并行提供了标准Concurrent库, Java7引入forkjoin并行计算，Java8流和 lamda 从Java 1.5以后增加了需要同步工具如CycicBariier, CountDownLatch 和 Sempahore，你应当优先使用这些同步工具，而不是去思考如何使用线程的wait和notify，通过BlockingQueue实现生产-消费的设计比使用线程的wait和notify要好得多 使用并发集合Collection而不是加了同步锁的集合. Java提供了 ConcurrentHashMap CopyOnWriteArrayList 和 CopyOnWriteArraySet以及BlockingQueue Deque and BlockingDeque五大并发集合 顺便也提一下 netty，在过去BIO的模式下，web服务器的并发是很低的，但是Netty使用了NIO，现在已经很少人选择 Apache和Tomcat 函数式编程也有翻身的趋势，现在使用aws的云服务进行大数据计算，并不需要多少分布式只是，可以聚焦在函数上 还有问题就是为什么并行编程很难 后续将逐步展开下面的话题 Reactive与Actors NIO Goroutine与Channel","categories":[{"name":"Concurrent & Parallelism","slug":"Concurrent-Parallelism","permalink":"http://hikings.github.io/categories/Concurrent-Parallelism/"}],"tags":[{"name":"Concurrent","slug":"Concurrent","permalink":"http://hikings.github.io/tags/Concurrent/"},{"name":"Parallelism","slug":"Parallelism","permalink":"http://hikings.github.io/tags/Parallelism/"}]},{"title":"Hugo  A fast and modern static website engine","slug":"2016-05-01-hugo-A-fast-and-modern-static-website-engine","date":"2016-05-01T04:10:33.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/05/01/2016-05-01-hugo-A-fast-and-modern-static-website-engine/","link":"","permalink":"http://hikings.github.io/2016/05/01/2016-05-01-hugo-A-fast-and-modern-static-website-engine/","excerpt":"Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。快速开始安装Hugo1. 二进制安装（推荐：简单、快速）到 Hugo Releases 下载对应的操作系统版本的Hugo二进制文件（hugo或者hugo.exe）","text":"Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。快速开始安装Hugo1. 二进制安装（推荐：简单、快速）到 Hugo Releases 下载对应的操作系统版本的Hugo二进制文件（hugo或者hugo.exe）Mac下直接使用 Homebrew 安装： brew install hugo2. 源码安装源码编译安装，首先安装好依赖的工具： GitMercurialGo 1.3+ (Go 1.4+ on Windows)设置好 GOPATH 环境变量，获取源码并编译： 12$ export GOPATH=$HOME/go$ go get -v github.com/spf13/hugo 源码会下载到 $GOPATH/src 目录，二进制在 $GOPATH/bin/ 如果需要更新所有Hugo的依赖库，增加 -u 参数： 1$ go get -u -v github.com/spf13/hugo 生成站点使用Hugo快速生成站点，比如希望生成到 /path/to/site 路径： 1$ hugo new site /path/to/site 这样就在 /path/to/site 目录里生成了初始站点，进去目录： 1$ cd /path/to/site 站点目录结构： ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml 创建文章创建一个 about 页面： 1$ hugo new about.md about.md 自动生成到了 content/about.md ，打开 about.md 看下： 内容是 Markdown 格式的，+++ 之间的内容是 TOML 格式的，根据你的喜好，你可以换成 YAML 格式（使用 — 标记）或者 JSON 格式。 创建第一篇文章，放到 post 目录，方便之后生成聚合页面。 1$ hugo new post/first.md 打开编辑 post/first.md ： 安装皮肤到 皮肤列表 挑选一个心仪的皮肤，比如你觉得 Hyde 皮肤不错，找到相关的 GitHub 地址，创建目录 themes，在 themes 目录里把皮肤 git clone 下来： 创建 themes 目录12$ cd themes$ git clone https://github.com/spf13/hyde.git 运行Hugo在你的站点根目录执行 Hugo 命令进行调试： 1$ hugo server --theme=hyde --buildDrafts --watch 使用 –watch 参数可以在修改文章内容时让浏览器自动刷新。 浏览器里打开： http://localhost:1313部署假设你需要部署在 GitHub Pages 上，首先在GitHub上创建一个Repository，命名为：name.github.io （name替换为你的github用户名）。 在站点根目录执行 Hugo 命令生成最终页面： 1$ hugo --theme=hyde --baseUrl=&quot;http://name.github.io/&quot; 如果一切顺利，所有静态页面都会生成到 public 目录，将pubilc目录里所有文件 push 到刚创建的Repository的 master 分支。 123456$ cd public$ git init$ git remote add origin https://github.com/name/name.github.io.git$ git add -A$ git commit -m &quot;first commit&quot;$ git push -u origin master 浏览器里访问：http://name.github.io/","categories":[{"name":"Static-Website","slug":"Static-Website","permalink":"http://hikings.github.io/categories/Static-Website/"}],"tags":[{"name":"GoLang","slug":"GoLang","permalink":"http://hikings.github.io/tags/GoLang/"},{"name":"Static-Website","slug":"Static-Website","permalink":"http://hikings.github.io/tags/Static-Website/"}]},{"title":"分布式领域论文目录","slug":"2016-04-09-Distributed-System-Classic-Paper","date":"2016-04-09T04:10:33.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/04/09/2016-04-09-Distributed-System-Classic-Paper/","link":"","permalink":"http://hikings.github.io/2016/04/09/2016-04-09-Distributed-System-Classic-Paper/","excerpt":"分布式领域论文目录 分布式领域论文译序 sql&amp;nosql年代记 SMAQ：海量数据的存储计算和查询 google论文系列","text":"分布式领域论文目录 分布式领域论文译序 sql&amp;nosql年代记 SMAQ：海量数据的存储计算和查询 google论文系列 google系列论文译序 The anatomy of a large-scale hypertextual Web search engine (译 zz) web search for a planet :the google cluster architecture(译) GFS：google文件系统 (译) MapReduce: Simplied Data Processing on Large Clusters (译) Bigtable: A Distributed Storage System for Structured Data (译) Chubby: The Chubby lock service for loosely-coupled distributed systems (译) Sawzall:Interpreting the Data–Parallel Analysis with Sawzall (译 zz) Pregel: A System for Large-Scale Graph Processing (译) Dremel: Interactive Analysis of WebScale Datasets(译zz) Percolator: Large-scale Incremental Processing Using Distributed Transactions and Notifications(译zz) MegaStore: Providing Scalable, Highly Available Storage for Interactive Services(译zz) Case Study GFS: Evolution on Fast-forward (译) Google File System II: Dawn of the Multiplying Master Nodes Tenzing - A SQL Implementation on the MapReduce Framework (译) F1-The Fault-Tolerant Distributed RDBMS Supporting Google’s Ad Business Elmo: Building a Globally Distributed, Highly Available Database PowerDrill：Processing a Trillion Cells per Mouse Click Google-Wide Profiling:A Continuous Profiling Infrastructure for Data Centers Spanner: Google’s Globally-Distributed Database(译zz) Dapper, a Large-Scale Distributed Systems Tracing Infrastructure(笔记) Omega: flexible, scalable schedulers for large compute clusters CPI2: CPU performance isolation for shared compute clusters Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams(译) F1: A Distributed SQL Database That Scales MillWheel: Fault-Tolerant Stream Processing at Internet Scale(译) B4: Experience with a Globally-Deployed Software Defined WAN The Datacenter as a Computer Google brain-Building High-level Features Using Large Scale Unsupervised Learning Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing(译zz) Large-scale cluster management at Google with Borg google系列论文翻译集(合集) 分布式理论系列译序 分布式理论系列译序 Appraising Two Decades of Distributed Computing Theory Research A brief history of Consensus_ 2PC and Transaction Commit (译) 拜占庭将军问题 (译) Impossibility of distributed consensus with one faulty process (译) Leases：租约机制 (译) Time Clocks and the Ordering of Events in a Distributed System(译) 关于Paxos的历史 The Part Time Parliament (译 zz) How to Build a Highly Available System Using Consensus(译) Paxos Made Simple (译) Paxos Made Live - An Engineering Perspective(译) 2 Phase Commit(译) Consensus on Transaction Commit(译) Why Do Computers Stop and What Can Be Done About It?(译) On Designing and Deploying Internet-Scale Services(译) Single-Message Communication(译) Life beyond Distributed Transactions:an Apostate’s Opinion(译zz) Implementing fault-tolerant services using the state machine approach 17. Problems, Unsolved Problems and Problems in Concurrency 18. Hints for Computer System Design 19. Self-stabilizing systems in spite of distributed control 20. Wait-Free Synchronization 21. White Paper Introduction to IEEE 1588 &amp; Transparent Clocks 22. Unreliable Failure Detectors for Reliable Distributed Systems 23. Life beyond Distributed Transactions:an Apostate’s Opinion(译zz) 24. Distributed Snapshots: Determining Global States of a Distributed System --Leslie Lamport 25. Virtual Time and Global States of Distributed Systems 26. Timestamps in Message-Passing Systems That Preserve the Partial Ordering 27. Fundamentals of Distributed Computing:A Practical Tour of Vector Clock Systems 28. Knowledge and Common Knowledge in a Distributed Environment 29. Understanding Failures in Petascale Computers 30. Why Do Internet services fail, and What Can Be Done About It? 31. End-To-End Arguments in System Design 32. Rethinking the Design of the Internet: The End-to-End Arguments vs. the Brave New World The Design Philosophy of the DARPA Internet Protocols(译zz)34. Uniform consensus is harder than consensus 35. Paxos made code - Implementing a high throughput Atomic Broadcast RAFT:In Search of an Understandable Consensus Algorithm 分布式理论系列论文翻译集(合集) 数据库理论系列 A Relational Model of Data for Large Shared Data Banks –E.F.Codd 1970 SEQUEL：A Structured English Query Language 1974 Implentation of a Structured English Query Language 1975 A System R: Relational Approach to Database Management 1976 Granularity of Locks and Degrees of Consistency in a Shared DataBase –Jim Gray 1976 Access Path Selection in a RDBMS 1979 The Transaction Concept:Virtues and Limitations –Jim Gray 2pc-2阶段提交：Notes on Data Base Operating Systems –Jim Gray 3pc-3阶段提交：NONBLOCKING COMMIT PROTOCOLS MVCC：Multiversion Concurrency Control-Theory and Algorithms –1983 ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging-1992 A Comparison of the Byzantine Agreement Problem and the Transaction Commit Problem –Jim Gray A Formal Model of Crash Recovery in a Distributed System - Skeen, D. Stonebraker What Goes Around Comes Around - Michael Stonebraker, Joseph M. Hellerstein Anatomy of a Database System -Joseph M. Hellerstein, Michael Stonebraker Architecture of a Database System(译zz) 大规模存储与计算(NoSql理论系列) Towards Robust Distributed Systems：Brewer’s 2000 PODC key notes CAP理论 Harvest, Yield, and Scalable Tolerant Systems 关于CAP BASE模型：BASE an Acid Alternative 最终一致性 可扩展性设计模式 可伸缩性原则 NoSql生态系统 scalability-availability-stability-patterns The 5 Minute Rule and the 5 Byte Rule (译) The Five-Minute Rule Ten Years Later and Other Computer Storage Rules of Thumb The Five-Minute Rule Ten Years Later and Other Computer Storage Rules of Thumb The Five-Minute Rule 20 Years Later(and How Flash Memory Changes the Rules) 关于MapReduce的争论 MapReduce：一个巨大的倒退 MapReduce：一个巨大的倒退(II) MapReduce和并行数据库，朋友还是敌人？(zz) MapReduce and Parallel DBMSs-Friends or Foes (译) MapReduce:A Flexible Data Processing Tool (译) A Comparision of Approaches to Large-Scale Data Analysis (译) MapReduce Hold不住？(zz) Beyond MapReduce：图计算概览 Map-Reduce-Merge: simplified relational data processing on large clusters MapReduce Online Graph Twiddling in a MapReduce World Spark: Cluster Computing with Working Sets Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing Big Data Lambda Architecture The 8 Requirements of Real-Time Stream Processing The Log: What every software engineer should know about real-time data’s unifying abstraction Lessons from Giant-Scale Services 基本算法和数据结构 大数据量，海量数据处理方法总结 Consistent Hashing And Random Trees Merkle Trees Scalable Bloom Filters Introduction to Distributed Hash Tables B-Trees and Relational Database Systems The log-structured merge-tree (译) lock free data structure lock free algorithm Data Structures for Spatial Database Gossip The Graph Traversal Pattern 基本系统和实践经验 MySQL索引背后的数据结构及算法原理 Dynamo: Amazon’s Highly Available Key-value Store (译zz) Cassandra - A Decentralized Structured Storage System (译zz) PNUTS: Yahoo!’s Hosted Data Serving Platform (译zz) Yahoo!的分布式数据平台PNUTS简介及感悟(zz) LevelDB：一个快速轻量级的key-value存储库(译) LevelDB理论基础 LevelDB：实现(译) LevelDB SSTable格式详解 LevelDB Bloom Filter实现 Sawzall原理与应用 Storm原理与实现 Designs, Lessons and Advice from Building Large Distributed Systems Challenges in Building Large-Scale Information Retrieval Systems Experiences with MapReduce, an Abstraction for Large-Scale Computation Taming Service Variability,Building Worldwide Systems,and Scaling Deep Learning Large-Scale Data and Computation:Challenges and Opportunitis Achieving Rapid Response Times in Large Online Services The Tail at Scale(译) How To Design A Good API and Why it Matters Event-Based Systems:Architect’s Dream or Developer’s Nightmare? Autopilot: Automatic Data Center Management 其他辅助系统 The ganglia distributed monitoring system:design, implementation, and experience Chukwa: A large-scale monitoring system Scribe : a way to aggregate data and why not, to directly fill the HDFS? Benchmarking Cloud Serving Systems with YCSB Dynamo Dremel ZooKeeper Hive 简述 Hadoop Reading List The Hadoop Distributed File System(译) HDFS scalability:the limits to growth(译) Name-node memory size estimates and optimization proposal. HBase Architecture(译) HFile：A Block-Indexed File Format to Store Sorted Key-Value Pairs HFile V2 Hive – A Petabyte Scale Data Warehouse Using Hadoop HIVE RCFile高效存储结构 Hive - A Warehousing Solution Over a Map-Reduce Framework Hive – A Petabyte Scale Data Warehouse Using Hadoop HIVE RCFile高效存储结构 ZooKeeper: Wait-free coordination for Internet-scale systems The life and times of a zookeeper Avro: 大数据的数据格式(zz) Apache Hadoop Goes Realtime at Facebook (译) Hadoop平台优化综述(zz) The Anatomy of Hadoop I/O Pipeline (译) Hadoop公平调度器指南(zz) 下一代Apache Hadoop MapReduce Apache Hadoop 0.23 深入理解计算机系统 其他 On Computable Numbers with an Application to the Entscheidungsproblem-1936.5.28-A.M.Turing The First Draft Report on the EDVAC-1945.6.30-John von Neumann Reflections on Trusting Trust –Ken Thompson Who Needs an Architect? Go To statements considered harmfull –Edsger W.Dijkstra No Silver Bullet Essence and Accidents of Software Engineering –Frederick P. Brooks phylips@bmy http://duanple.blog.163.com/blog/static/709717672011330101333271/ http://blog.nosqlfan.com/html/1647.html 分布式系统阅读清单 转载请注明作者：phylips@bmy 2011-4-30 出处：http://duanple.blog.163.com/blog/static/709717672011330101333271/ 再推荐一个相关文章：http://blog.nosqlfan.com/html/1647.html","categories":[{"name":"Distributed-Computer-Systems","slug":"Distributed-Computer-Systems","permalink":"http://hikings.github.io/categories/Distributed-Computer-Systems/"}],"tags":[{"name":"DS","slug":"DS","permalink":"http://hikings.github.io/tags/DS/"},{"name":"PAPER","slug":"PAPER","permalink":"http://hikings.github.io/tags/PAPER/"},{"name":"GoLang","slug":"GoLang","permalink":"http://hikings.github.io/tags/GoLang/"}]},{"title":"一个非常优秀的分布式系统课程，也可用来学习Go语言","slug":"2016-04-21-MIT-Distributed-Computer-Systems-Engineering","date":"2016-04-09T04:10:33.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/04/09/2016-04-21-MIT-Distributed-Computer-Systems-Engineering/","link":"","permalink":"http://hikings.github.io/2016/04/09/2016-04-21-MIT-Distributed-Computer-Systems-Engineering/","excerpt":"6.824是MIT的分布式系统课程，学习语言和知识还是动手才能发现不足，这个课程非常棒，既包含了分布式领域最实用的理论又结合了实验进行演练。 在此做个推荐。 - 以前的课程实现是Cpp实现一个分布式文件系统，可以看12年的课程。 - 现在课程进行了刷新，并且语言更新为Go，现在正在切换成Go语言开发。从新学习一次这个课程和相关的Papers。","text":"6.824是MIT的分布式系统课程，学习语言和知识还是动手才能发现不足，这个课程非常棒，既包含了分布式领域最实用的理论又结合了实验进行演练。 在此做个推荐。 - 以前的课程实现是Cpp实现一个分布式文件系统，可以看12年的课程。 - 现在课程进行了刷新，并且语言更新为Go，现在正在切换成Go语言开发。从新学习一次这个课程和相关的Papers。 Lab 1: MapReduce 实现任务分配 ，支持将任务分发给线程 Lab 2: Primary/Backup Key/Value Service 考查主从备份的实现 Lab 3: Paxos-based Key/Value Service 实现paxos协议 利用paxos协议，实现一个多副本的服务器，保证副本间的一致 Lab 4: Sharded Key/Value Service 实现数据分片服务器 数据分片的一致性：节点的加入，离开等，导致数据分片的移动 Lab 5: Persistence 数据持久化 重启支持：能够在磁盘数据丢失与未丢失情况下，进行恢复 6.824 - Spring 2016 –MIT 6.824主页","categories":[{"name":"Distributed-Computer-Systems","slug":"Distributed-Computer-Systems","permalink":"http://hikings.github.io/categories/Distributed-Computer-Systems/"}],"tags":[{"name":"DS","slug":"DS","permalink":"http://hikings.github.io/tags/DS/"},{"name":"PAPER","slug":"PAPER","permalink":"http://hikings.github.io/tags/PAPER/"},{"name":"GoLang","slug":"GoLang","permalink":"http://hikings.github.io/tags/GoLang/"}]},{"title":"互联网公司的敏锐嗅觉","slug":"2016-03-13-learn-from-mi-tencent","date":"2016-03-13T14:16:25.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/03/13/2016-03-13-learn-from-mi-tencent/","link":"","permalink":"http://hikings.github.io/2016/03/13/2016-03-13-learn-from-mi-tencent/","excerpt":"当大家开心的观赏人机对战时，隐隐感觉不妥，细的一想 兴趣盎然的通知家父正在直播Alphago对李世石第三战，家父问：“是5台转播么？”，我一时无语 传统媒体，电视台毫无反应，大概是感觉没有价值，如果是里程碑的转择点是否还有价值呢 自然的在电视上没有办法看直播","text":"当大家开心的观赏人机对战时，隐隐感觉不妥，细的一想 兴趣盎然的通知家父正在直播Alphago对李世石第三战，家父问：“是5台转播么？”，我一时无语 传统媒体，电视台毫无反应，大概是感觉没有价值，如果是里程碑的转择点是否还有价值呢 自然的在电视上没有办法看直播 小米公司是非常积极的，一早就邀请了泰斗聂老做讲解 腾讯公司更为积极，安排团队去了韩国，并且邀请古力等重量级做讲解 老牌互联网网易、新浪自然不会被落下 那我的疑问，其他人没有反应的判断依据都有哪些，或者不屑一顾，去还是不去是否是个问题？ 以电视媒体极不友好的体验，终究只是多活些时日罢了，我等用脚投票","categories":[{"name":"AI","slug":"AI","permalink":"http://hikings.github.io/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://hikings.github.io/tags/AI/"}]},{"title":"2:0 AlphaGo vs 李世石","slug":"2016-03-10-wow-alphago","date":"2016-03-10T13:48:57.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/03/10/2016-03-10-wow-alphago/","link":"","permalink":"http://hikings.github.io/2016/03/10/2016-03-10-wow-alphago/","excerpt":"机器虽然不完美，已经可以下出优美的棋 “我们的棋用不了多少年就会被遗忘，只有武宫的棋才会流芳百世。” – 藤泽秀行 刚好周末，用了整个周末来学习AlphaGo，最成功的太太也一起参与学习 无论结果如何，大家都得其所，这是个很好的结局 - 谷歌团队如愿以偿，发现了程序缺陷 - 职业棋手已经正视AI，并成为共识 - AI将来有能力成为职业棋手学习的新途径，人最的优势是学习，过去和古人学，现在与AI学习，有何不可？ 两天的全程看了直播，一场聂老讲解，一场古力讲解 本以为盘面越小机器越有优势，结果是： 用在收官是正确的，关子几乎完美，无错 本以为善于局部战斗，结果战斗力非常差 本以为布局会比较差和棋型很丑，结果完全相反，第三局的棋型极为优美 原来推测的自学习会是出错的原因，在第四局上得了证实 选择树的决策算法不足，在出现了未预计的算法时，不知所措，被逆转 在被逆转以后，连续走出了极低效率的棋 以我追求的的围棋观开头，AI和围棋我都是业余水平 AI 工作中用不上，学习过一点公开课和开源项目，顺便推荐吴恩达（aka. Andrew Ng）的机器学习公开课 围棋只是业余水平 对于AI(DL)/围棋我都不是利益相关人 我不属于相关了商业和派系中 围棋是个奇怪的东西，成人被10来岁的小孩欺负是很正常是现象 基本上15岁没入段，这辈子也就这样了，武宫如此，吴清源如此。 专业棋手大多5岁学期、10余岁出入段，20余岁已是世界冠军","text":"机器虽然不完美，已经可以下出优美的棋 “我们的棋用不了多少年就会被遗忘，只有武宫的棋才会流芳百世。” – 藤泽秀行 刚好周末，用了整个周末来学习AlphaGo，最成功的太太也一起参与学习 无论结果如何，大家都得其所，这是个很好的结局 - 谷歌团队如愿以偿，发现了程序缺陷 - 职业棋手已经正视AI，并成为共识 - AI将来有能力成为职业棋手学习的新途径，人最的优势是学习，过去和古人学，现在与AI学习，有何不可？ 两天的全程看了直播，一场聂老讲解，一场古力讲解 本以为盘面越小机器越有优势，结果是： 用在收官是正确的，关子几乎完美，无错 本以为善于局部战斗，结果战斗力非常差 本以为布局会比较差和棋型很丑，结果完全相反，第三局的棋型极为优美 原来推测的自学习会是出错的原因，在第四局上得了证实 选择树的决策算法不足，在出现了未预计的算法时，不知所措，被逆转 在被逆转以后，连续走出了极低效率的棋 以我追求的的围棋观开头，AI和围棋我都是业余水平 AI 工作中用不上，学习过一点公开课和开源项目，顺便推荐吴恩达（aka. Andrew Ng）的机器学习公开课 围棋只是业余水平 对于AI(DL)/围棋我都不是利益相关人 我不属于相关了商业和派系中 围棋是个奇怪的东西，成人被10来岁的小孩欺负是很正常是现象 基本上15岁没入段，这辈子也就这样了，武宫如此，吴清源如此。 专业棋手大多5岁学期、10余岁出入段，20余岁已是世界冠军 谣言四起时，我的态度 有不服，开始约战 赞！人怎么可以害怕机器 有阴谋论，开始爆谣言和黑幕。 各种解读开始出现，有一些在长篇解读中掺杂“有特殊目的的观点” 这个不能接受 媒体大肆渲染，“天网”，“终结者”，“人类末日” 最糟糕 攻击算法 不服可以约战嘛, 机器和机器也可以比 夸大算法复杂度 不排除围棋中很多不可能的下法，简单的19*19的计算不科学 。。。 说正题 不论这次胜负结果，不论是否有商业因素，不论算法成熟这次比赛的积极意义要更多 之前的 新旧三架马车 论文，触发的发展，已经使业内和普通百姓都开始受益 这次即使是炒作，但引起了包括普通人的关注，那么就是前进 我有一箩筐的问题 情感，精力上机器和人有本质区别 人有爱有憎有贪，这时多半要丢棋，昨天和今天李世石都出了错 机器不会累，人的耐力有限。已聂棋圣为例，擂台赛的时候他都需要吸氧，多次因为缺氧出昏招丢棋 人与机器学习的过程类似，甚至机器还有记忆优势 人学棋大致是模仿（看高手、打谱），水平提升快还得有水平相当的对手，下多了慢慢有自己的棋风 慢慢的最优解会成为定式，这个机器的核心优势，机器基本不会出错，会小部分“大雪崩” 变化的已经可以是业余高手，每年新出很多“定式”，人不可能学穷，但机器可以 机器的棋风是什么？ 100盘以后，人差不多会有棋风，这是人的天赋，机器是否有棋风？ 算法是否经得起考验 AlphaGo 有一方面通过缩小范围来简化复杂度来判断，一方面通过局面的优劣来判断，最终如何处理是上述两个结果的加权处理结果。那么问题来了，论文中的最终结果判断还显得粗糙，或者保密了（论文中一个有趣的结论是：两个大脑取平均的结果比依赖两者各自得出的结果都要好很多） AlphaGo 都是谁参与，资源是否充足 项目最怕的就是预算，一分钱难死英雄汉 AI的比赛多见是大赛，胜利者可以拿奖金。但即便是1M美金，又怎么够呢？ AlphaGo阵容豪华，如果要形容就是类似运动比赛中的“梦之队”，人员和资源令人羡慕，Facebook的黑暗森林投入要小很多 AlphaGo 项目主页 Nature 论文 人类下棋与机器学习有什么异同？ 人如何决策下一步怎么走？ 选择一个效率最高的貉子（价值最大化） 或者选择一个最紧急的落子（急手） 需要有限的时间内完成选择（决策） 上述决策可能以来于 计算、习惯、直觉、常识 习惯有些走法是比如的但与性格冲突，性格强或者有最求的棋手是不会选择的 直觉是棋感 常识是定式、征子等经验总结 人力怎么计算，可以计算多少步？ 吴清源在对木谷实(日本著名围棋大师)的回忆中写道：即便是对业余棋手下让九子的指导棋，他一般每局也要用半天以上。……曾问其为何长考的理由。他回答说，”他首先在作为直感而浮现于眼前的四、五手中，从最不可能成立的一手开始，一子一子地往下计算。”吴清源：”但是，除了中盘的绞杀和收官以外，其他的地方无论如何也是算不尽的。况且，对方若在自己计算范围外的地方打下一手的话，那么一切还得再从零开始算。 与木谷实相反，我首先在最早浮现于眼前的几手中，从最有可能成立的一手开始算，如这一手不行，再考虑另一手。 一般业余水平可以算 2/3 种变化，10余步 业务高手可以算 30步以上 职业选手可以算 60步以上 传闻：日本著名棋手加藤的算路很深，一次在拆解一个变化时停下来说：“不行，下面第三十四手有一个双打，黑没法两全”。当时坂田荣男在边上说：“可以，接着走到第六十二手的时候可以反吃回来。” 算 3 步就可以是象棋高手，聂卫平当年去日本中日擂台，船上无聊先学国际象棋，结果杀翻所有象棋代表 棋力相差有多大 基于围棋段位是比赛得出，不是考试得出，所以业余同段的差距非常大 历史上没有对弈过的棋手，也不能对比，更多的是评价对围棋的影响而不是是不是世界冠军，比如木谷实先生 职业棋手可让业余棋手的子数 九段最高，实际上水平差距也很大，都是九段更多看是否得到头衔，比如“本因坊”、“名人” 职业初段有可能赢九段 “一子三段”，段位上相差三段，就可以让一子 职业棋手可让业余棋手的子数 业余每段一子 业余很难升5以上，业余5段以上的水平很难真正评价，棋力堪比职业也是可能的 职业对业余7段 让先 人类学棋与机器学习有什么异同？ AlphaGo 自对局是否有意义 对局数的边际效应，可以确定并不是越多越好，但边界点不明确 自然人自对局对棋力提升很有限，更多可能是精神分裂 AlphaGo 是否过分的利用的集群能力 比我想象的要好，赞！引用论文： AlphaGo uses an asynchronous multi-threaded search that executes simulations on CPUs, and computes policy and value networks in parallel on GPUs.The final version of AlphaGo used 40 search threads, 48 CPUs, and 8 GPUs. We also implemented a distributed version of AlphaGo that exploited multiple machines, 40 search threads, 1,202 CPUs and 176 GPUs. tbd 怎么看对局 还没有打谱，看到第二局的布局，有点像 “中国流” tbd AlphaGo 的机理 周末读完论文补充 tbd AlphaGo 中可以借鉴什么 周末读完论文补充 tbd AI 在哪些领域已经影响生活 模式识别相对成熟 图像识别 语音识别，NLP 自动控制 仿真系统 搜索和购物中的推荐系统 what-you-wanted-to-know-about-ai/ 谷歌AlphaGo 团队 “人机大战”围观必读：谷歌AlphaGo背后的霸道总裁和科学狂人 谷歌详解的人机围棋大战的意义材料翻译 我的学棋经历首日听闻后期逆转AlphaGo赢了，认为只是发挥失误。次日又闻已经 2:0 领先心情又点复杂，想整理一下，首先想起的是这句，“我们的棋用不了多少年就会被遗忘，只有武宫的棋才会流芳百世。” – 藤泽秀行我自己9岁学棋，师从家父。巅峰时期居然也是初中时期，限于自己的天赋，水平一直不入流 最大的原因是苦于找不到合适的对手，几年下来，水平反而下降了 初一杀下了高三的学长拿了校冠军、 高中时候刚好遇上省运动会，我按告示去学生会报名时，老师一脸诧异，大概没想到有愣头小子来报名 实际上已经内定了三位高三的学长，我去报名了也不能这样了，约了下课时候比一场，结果2:0两盘中盘胜拿了三席 那时候主学小林光一和武宫正树，打谱基本就是这两位先生，尤其爱星位 现实很残酷，走小林基本没输过，走武宫多半输棋，跌倒无数次还是不能回头， 代表学校出战时，试探手过后，发现对手略弱，心思就起来了，想赢得漂亮，结果反而输了比赛 推荐阅读AlphaGo 项目主页 Nature 论文 Nature 报道 Dan Maas 对论文的浅显总结 Google工程师木遥的观点 what-you-wanted-to-know-about-ai/ “人机大战”围观必读：谷歌AlphaGo背后的霸道总裁和科学狂人谷歌详解的人机围棋大战的意义材料翻译","categories":[{"name":"AI","slug":"AI","permalink":"http://hikings.github.io/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://hikings.github.io/tags/AI/"}]},{"title":"软件进步方式的巨大变化","slug":"2016-03-09-soft-repo","date":"2016-03-09T13:38:22.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/03/09/2016-03-09-soft-repo/","link":"","permalink":"http://hikings.github.io/2016/03/09/2016-03-09-soft-repo/","excerpt":"只需要提交代码，喝杯咖啡回来，数台或成百上千的集群已经运行中 几年以前都不敢想象，那时候系统不成熟，都是开发人员直接上机房","text":"只需要提交代码，喝杯咖啡回来，数台或成百上千的集群已经运行中 几年以前都不敢想象，那时候系统不成熟，都是开发人员直接上机房 曾经在没有DVD的时代，带着5/6张光盘从裸机开始部署系统 数百页的安装手册，其实不算什么 糟糕的是一个命令打错，就得从新来过，但是 ` 和 ‘ ， l 和 1 实在是区分不清啊 - 硬件的稳定和兼容，不把开发折腾死不罢休，启初是 SUN 机器，除了贵点一切都好。卖了以后公司想切换结果成了安腾的小白鼠。。。。 现在通过虚拟化镜像文件，搭配Orchestration系统一起使用，一个描述文件，完成所有工作。赞！！！ 这周趁 MAC 重装，在家顺便就一起捣腾了Windows、 Ubuntu（基于windows上的vm player虚拟）、Debian（NAS），我把自己分时调度到各个的键盘上^_^，体会也是相比以前一切简化太多 软件安装方式的巨大变化 搭建 OS X、Linux、Windows 几套环境，一般都不敢想 过去很不方便，需要手动去网站下载软件，然后一步一步安装，经常遇到网络问题，一阻塞就是半天。 这个所有程序员的痛，随着UNIX/LINUX文化的影响，大家逐渐把 yum、apt 这样安装软件的方式普及到了各个环境和语言 当然 Windows 只算有半个, Windows 10 引入的 应用商店，第一次有了官方的应用下载市场 CPP仍旧高傲的一个人玩 各个仓库 MAC OS : homebrew Linux : apt (Ubuntu、Debian)、yum、yast Windows : 微软大爷不提供 CPP : 从来都是内存管理、连接池、线程池、日志都得自己写 JAVA : maven Python : pip Ruby : gem Nodejs : npm Sublime : Packge Control Eclipse : Plugins 问题: 虽然好了很多，但是包管理软件依旧存在问题，还是很混乱 一种语言有多个包管理 ant -&gt; maven - gradle -&gt; sbt , easyinstall , pip 包仓库里的版本过旧，导致很多场景必须下载，从代码编译，又回到了原始社会","categories":[{"name":"Package-Management","slug":"Package-Management","permalink":"http://hikings.github.io/categories/Package-Management/"}],"tags":[{"name":"Package-Management","slug":"Package-Management","permalink":"http://hikings.github.io/tags/Package-Management/"},{"name":"Deployment-Management","slug":"Deployment-Management","permalink":"http://hikings.github.io/tags/Deployment-Management/"}]},{"title":"从折腾开始，我的计划","slug":"2016-03-05-beginning-and-list","date":"2016-03-05T12:18:43.000Z","updated":"2016-09-16T09:17:24.000Z","comments":true,"path":"2016/03/05/2016-03-05-beginning-and-list/","link":"","permalink":"http://hikings.github.io/2016/03/05/2016-03-05-beginning-and-list/","excerpt":"写在开始 遇坑，从折腾开始，新旧对比中受益匪浅 在用的版本 Yosemite，去年就发布了el capitan新版本，多年的遇坑经验让我克制住没有升级，这次不经意间发现store中推送，应该是stable版本了吧 兴冲冲地的下载，安装，悲剧发生 磁盘加密 Filevault 本来是个10多年的老特性，一直默默无闻，结果 Yosemite 版本中为了刷存在感变成缺省选项，没细看就中招了O(∩_∩)O 这次升级自然是吧了，并且在过程中，不能取消，不能升级版本，不能格式化磁盘 索性删除磁盘重装，并且同时处理 OS X el capitan, Windows , Ubuntu 三个环节，巨坑的开始，前后折腾一个星期 可迁移的环境 自动搭环境的项目 多版本 可并存 可替换 可切换","text":"写在开始 遇坑，从折腾开始，新旧对比中受益匪浅 在用的版本 Yosemite，去年就发布了el capitan新版本，多年的遇坑经验让我克制住没有升级，这次不经意间发现store中推送，应该是stable版本了吧 兴冲冲地的下载，安装，悲剧发生 磁盘加密 Filevault 本来是个10多年的老特性，一直默默无闻，结果 Yosemite 版本中为了刷存在感变成缺省选项，没细看就中招了O(∩_∩)O 这次升级自然是吧了，并且在过程中，不能取消，不能升级版本，不能格式化磁盘 索性删除磁盘重装，并且同时处理 OS X el capitan, Windows , Ubuntu 三个环节，巨坑的开始，前后折腾一个星期 可迁移的环境 自动搭环境的项目 多版本 可并存 可替换 可切换 TODO LIST 微服务 自动搭环境的项目 Surface 前路如何 使用描述语言自动拼图 我这一周遇到的坑 mac 为什么缺省打开了 filevault 选项，非常坑人 新的mac os 缺省的版本的对缺省库进行保护 windows 下的 ruby 很容易安装 类似 apt-get , npm , pip , gem 这样的软件仓库是进步 软件发布方式的变迁，安装包、OVF、 通过GIT的源码方式发布 osgi karaf , soa, micro service ? new open source last year why mac book perference not updated ? huawei matebook use m3 chip, is right? 定位是否准确，还是赌博， 有待观察 打印的选项设置，打印纸张限制的坑","categories":[{"name":"note","slug":"note","permalink":"http://hikings.github.io/categories/note/"}],"tags":[{"name":"note","slug":"note","permalink":"http://hikings.github.io/tags/note/"}]},{"title":"Hexo mixture","slug":"2016-03-04-build-hexo-and-command","date":"2016-03-04T13:48:45.000Z","updated":"2017-09-27T15:08:03.000Z","comments":true,"path":"2016/03/04/2016-03-04-build-hexo-and-command/","link":"","permalink":"http://hikings.github.io/2016/03/04/2016-03-04-build-hexo-and-command/","excerpt":"搭建流程 需要两个分支或者仓库，分别用来保存生成结果和源文件 创建仓库，xx.github.io 创建两个分支：master 与 dev 或者两个仓库 git clone 通过Git bash依次执行 1234npm install -g hexo-clihexo init$ cd &lt;folder&gt;$ npm install 修改_config.yml中的deploy参数，分支应为master 执行hexo generate -d生成网站并部署到GitHub上 执行git add .、git commit -m “…”、git push 提交源文件到dev分之或者源码仓库","text":"搭建流程 需要两个分支或者仓库，分别用来保存生成结果和源文件 创建仓库，xx.github.io 创建两个分支：master 与 dev 或者两个仓库 git clone 通过Git bash依次执行 1234npm install -g hexo-clihexo init$ cd &lt;folder&gt;$ npm install 修改_config.yml中的deploy参数，分支应为master 执行hexo generate -d生成网站并部署到GitHub上 执行git add .、git commit -m “…”、git push 提交源文件到dev分之或者源码仓库 日常修改 在本地对博客进行修改 分别发布到两个仓库（用脚本自动化） \b恢复环境 当重装电脑之后，或者想在其他电脑上修改博客，可以使用下列步骤： 使用git clone git@github.com:CrazyMilk/CrazyMilk.github.io.git拷贝仓库（默认分支为hexo）； 在本地新拷贝的xx.github.io文件夹下通过Git bash依次执行下列指令（记得，不需要hexo init这条指令）。123npm install -g hexo-cli$ cd &lt;folder&gt;$ npm install Hexo! documentation for more info. find the answer in troubleshooting ask on GitHub. 常见问题发布后没有更新 使用 hexo clean 命令, 然后重新生成 删除 .deploy 目录, 然后重新发布 目录结构12345678hexo/ |- node_modules/ # hexo需要的模块，不需要上传GitHub |- themes/ # 主题文件，需要上传GitHub的dev分支 |- sources/ # 博文md文件，需要上传GitHub的dev分支 |- public/ # 生成的静态页面，由hexo deploy自动上传到gh-page分支 |- package.json # 记录hexo需要的包信息，不需要上传GitHub |- _config.yml # 全局配置文件，需要上传GitHub的dev分支 |- .gitignore # hexo生成默认的.gitignore，它已经配置好了不需要上传的hexo文件 Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 功能扩展支持Flowchart安装 hexo-filter-flowchart 1npm install --save hexo-filter-flowchart UsageThis plugin is based on flowchart.js, so you can defined the chart as follow: Config_config.yml:123flowchart: # raphael: # optional, the source url of raphael.js # flowchart: # optional, the source url of flowchart.js 支持Flowcharthexo-filter-sequencehttps://github.com/bubkoo/hexo-filter-sequence npm: Package Quality Generate UML sequence diagrams for Hexo.Install1npm install --save hexo-filter-sequence UsageThis plugin is based on js-sequence-diagrams, so you can defined the chart as follow: ConfigIn your site’s _config.yml:```sequence: webfont: # optional, the source url of webfontloader.jssnap: # optional, the source url of snap.svg.jsunderscore: # optional, the source url of underscore.jssequence: # optional, the source url of sequence-diagram.jscss: # optional, the url for css, such as hand drawn theme options: theme: css_class:```st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{\"theme\":\"simple\",\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks!{\"theme\":\"simple\",\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"sequence-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"sequence-0-options\").value)); var diagram = Diagram.parse(code); diagram.drawSVG(\"sequence-0\", options);","categories":[{"name":"BLOG","slug":"BLOG","permalink":"http://hikings.github.io/categories/BLOG/"}],"tags":[{"name":"BLOG","slug":"BLOG","permalink":"http://hikings.github.io/tags/BLOG/"}]}]}